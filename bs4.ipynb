{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "# import lxml\n",
    "\n",
    "with open('web/website.html', encoding='utf-8') as file: # Windows 시스템에서는 open() 함수가 파일을 읽을 때 cp1252 인코딩을 사용하므로 utf-8로 인코딩을 지정해야 함\n",
    "    contents = file.read()\n",
    "\n",
    "soup = BeautifulSoup(contents, 'html.parser')\n",
    "print(soup.title.string) \n",
    "print(soup.prettify()) # 들여쓰기를 사용하여 HTML 코드를 보기 좋게 출력\n",
    "\n",
    "print(soup.a) # content의 첫 번째 <a> 태그만 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"https://www.appbrewery.co/\">The App Brewery</a>, <a href=\"https://angelabauer.github.io/cv/hobbies.html\">My Hobbies</a>, <a href=\"https://angelabauer.github.io/cv/contact-me.html\">Contact Me</a>]\n"
     ]
    }
   ],
   "source": [
    "all_anchor_tag = soup.find_all(name='a') # 모든 <a> 태그를 리스트로 반환\n",
    "print(all_anchor_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The App Brewery\n",
      "My Hobbies\n",
      "Contact Me\n"
     ]
    }
   ],
   "source": [
    "for tag in all_anchor_tag:\n",
    "    print(tag.getText()) # <a> 태그의 내용을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.appbrewery.co/\n",
      "https://angelabauer.github.io/cv/hobbies.html\n",
      "https://angelabauer.github.io/cv/contact-me.html\n"
     ]
    }
   ],
   "source": [
    "for tag in all_anchor_tag:\n",
    "    print(tag.get('href')) # <a> 태그의 href 속성을 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"name\">Angela Yu</h1>\n"
     ]
    }
   ],
   "source": [
    "heading = soup.find(name='h1', id='name') # id 속성이 'name'인 <h1> 태그를 찾음\n",
    "print(heading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h3 class=\"heading\">Books and Teaching</h3>\n"
     ]
    }
   ],
   "source": [
    "section_heading = soup.find(name='h3', class_='heading') # class 속성이 'heading'인 <h3> 태그를 찾음 / class는 예약어 이므로 class_로 사용\n",
    "print(section_heading)\n",
    "# text만 가져오려면 print(section_heading.getText())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"https://www.appbrewery.co/\">The App Brewery</a>\n"
     ]
    }
   ],
   "source": [
    "company_url = soup.select_one(selector='p a') # selector: <p> 태그 아래에 있는 <a> 태그를 찾음 / 하나의 태그만 찾을 때 사용\n",
    "print(company_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 id=\"name\">Angela Yu</h1>\n"
     ]
    }
   ],
   "source": [
    "name = soup.select_one(selector='#name') # #, CSS 선택자를 사용하여 <p> 태그 아래에 있는 <a> 태그를 찾음 / 하나의 태그만 찾을 때 사용\n",
    "print(name) # selector는 생략 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h3 class=\"heading\">Books and Teaching</h3>, <h3 class=\"heading\">Other Pages</h3>]\n"
     ]
    }
   ],
   "source": [
    "heading = soup.select('.heading') # class 속성이 'heading'인 모든 <h3> 태그를 찾음 / 여러 태그를 찾을 때 사용\n",
    "print(heading)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# find 메서드: html 문서에서 첫번째로 발견된 특정 태그를 찾는다. \n",
    "# select_one : css 선택자를 사용하여 html 문서에서 첫 번째로 발견된 요소를 찾는다\n",
    "\n",
    "# select 메서더: css 선택자를 사용하여 html 문서에서 모든 일치하는 요소를 찾는다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'HTML 데이터 파싱'\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# 웹 페이지의 HTML 데이터를 가져옵니다.\n",
    "response = requests.get('https://example.com')  # 원하는 웹 페이지 URL\n",
    "html_data = response.text\n",
    "\n",
    "# BeautifulSoup을 사용하여 HTML을 파싱합니다.\n",
    "soup = BeautifulSoup(html_data, 'html.parser')\n",
    "\n",
    "# HTML의 첫 1000자를 출력해봅니다.\n",
    "print(soup.prettify()[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'페이지의 모든 <a> 태그(링크)를 찾습니다'\n",
    "\n",
    "all_links = soup.find_all('a')\n",
    "for link in all_links:\n",
    "    print(link.get('href'))  # 링크의 href 속성 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 클래스가 포함된 <div> 태그를 찾습니다.\n",
    "\n",
    "specific_divs = soup.find_all('div', class_='example-class')\n",
    "for div in specific_divs:\n",
    "    print(div.text)  # div의 텍스트 내용 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'특정 ID가 적용된 요소를 찾습니다'\n",
    "\n",
    "specific_element = soup.find(id='example-id')\n",
    "print(specific_element.text)  # 해당 요소의 텍스트 내용 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'모든 <a> 태그의 링크를 추출합니다'\n",
    "\n",
    "links = soup.find_all('a')\n",
    "for link in links:\n",
    "    href = link.get('href')\n",
    "    if href:  # 링크가 존재하는지 확인\n",
    "        print(href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 텍스트를 포함하는 태그를 찾습니다.\n",
    "\n",
    "specific_text = soup.find_all(string='Example Domain')\n",
    "for text in specific_text:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"<img> 태그의 모든 'src' 속성 값을 추출합니다\"\n",
    "\n",
    "images = soup.find_all('img')\n",
    "for img in images:\n",
    "    print(img.get('src'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 태그의 자식 요소를 탐색합니다.\n",
    "\n",
    "parent_div = soup.find('div', class_='parent-class')\n",
    "children = parent_div.find_all('span')\n",
    "for child in children:\n",
    "    print(child.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSS 선택자를 사용하여 요소를 찾습니다.\n",
    "\n",
    "elements = soup.select('div.example-class > a')\n",
    "for element in elements:\n",
    "    print(element.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 링크 데이터를 파일로 저장합니다.\n",
    "\n",
    "with open('links.txt', 'w') as f:\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        if href:\n",
    "            f.write(href + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# 이미지 다운로드 예시\n",
    "for img in images:\n",
    "    img_url = img.get('src')\n",
    "    if img_url:\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open('image_name.jpg', 'wb') as handler:\n",
    "            handler.write(img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 미디어 파일 추출\n",
    "\n",
    "media_urls = []\n",
    "\n",
    "# 이미지\n",
    "images = soup.find_all('img')\n",
    "for img in images:\n",
    "    media_urls.append(img.get('src'))\n",
    "\n",
    "# 오디오\n",
    "audios = soup.find_all('audio')\n",
    "for audio in audios:\n",
    "    media_urls.append(audio.get('src'))\n",
    "\n",
    "# 비디오\n",
    "videos = soup.find_all('video')\n",
    "for video in videos:\n",
    "    media_urls.append(video.get('src'))\n",
    "\n",
    "# Source 태그 (오디오와 비디오의 소스)\n",
    "sources = soup.find_all('source')\n",
    "for source in sources:\n",
    "    media_urls.append(source.get('src'))\n",
    "\n",
    "# 미디어 파일 URL 출력\n",
    "for url in media_urls:\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 웹 사이트 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# 웹 페이지의 HTML 데이터를 가져옵니다.\n",
    "response = requests.get('https://news.ycombinator.com/news')\n",
    "\n",
    "# BeautifulSoup을 사용하여 HTML을 파싱합니다.\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 보기 좋게 HTML을 정리하여 출력합니다.\n",
    "print(soup.prettify()) \n",
    "\n",
    "yc_web_page = response.text\n",
    "\n",
    "soup = BeautifulSoup(yc_web_page, 'html.parser')\n",
    "soup.title\n",
    "\n",
    "articles = soup.find_all(name='a', class_='storylink')\n",
    "print(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 웹 페이지 가져오기\n",
    "# 웹 페이지의 HTML 소스를 가져오기 위해 requests 라이브러리를 사용합니다.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://finance.yahoo.com/\"  # 야후 파이낸스 URL\n",
    "response = requests.get(url)\n",
    "html_content = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. BeautifulSoup 객체 생성\n",
    "# HTML 소스를 파싱하여 BeautifulSoup 객체를 만듭니다.\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P: Medicare Advantage is new villain for hospitals\n"
     ]
    }
   ],
   "source": [
    "# 3. 특정 텍스트 가져오기\n",
    "# 화면에 가장 큰 글씨로 표시된 텍스트를 가져오기 위해 soup.find() 또는 soup.select_one()를 사용합니다. \n",
    "# 해당 텍스트는 특정 태그에 포함되어 있을 가능성이 높습니다. 여기서는 soup.find()로 접근해보겠습니다.\n",
    "headline = soup.find('h2', string=\"S&P: Medicare Advantage is new villain for hospitals\")\n",
    "print(headline.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S&P: Medicare Advantage is new villain for hospitals\n"
     ]
    }
   ],
   "source": [
    "# 태그와 클래스 또는 ID 속성을 기반으로 접근\n",
    "headline = soup.find('h2')  # 첫 번째 <h2> 태그를 찾음\n",
    "print(headline.get_text())  # 텍스트를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 기사 제목을 먼저 찾습니다.\n",
    "headline = soup.find('h2', string=\"S&P: Medicare Advantage is new villain for hospitals\")\n",
    "\n",
    "# 해당 제목의 부모 컨테이너(예: div 또는 article)를 찾습니다.\n",
    "# 부모 컨테이너 안에 이미지 태그가 있을 가능성이 높습니다.\n",
    "if headline:\n",
    "    parent_container = headline.find_parent()  # 또는 find_parent('div') 등 특정 태그로 부모를 지정할 수 있음\n",
    "    \n",
    "    # 부모 컨테이너 안에서 이미지를 찾습니다.\n",
    "    image = parent_container.find('img')\n",
    "    \n",
    "    if image:\n",
    "        image_url = image['src']\n",
    "        print(image_url)\n",
    "    else:\n",
    "        print(\"이미지를 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(\"제목을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 이미지 가져오기\n",
    "# 특정 이미지의 URL을 가져오기 위해 이미지가 포함된 태그를 찾아야 합니다. \n",
    "# 이미지 태그는 일반적으로 <img>이며, 이 태그의 src 속성을 사용하여 이미지를 가져옵니다.\n",
    "image = soup.find('img', alt=\"Image description related to the green image\")\n",
    "image_url = image['src']\n",
    "print(image_url)\n",
    "\n",
    "TypeError: 'NoneType' object is not subscriptable # 이미지가 없을 때 발생"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "image = soup.find('img', alt=\"Image description related to the green image\")\n",
    "if image:\n",
    "    image_url = image['src']\n",
    "    print(image_url)\n",
    "else:\n",
    "    print(\"이미지를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "image = soup.find('img')\n",
    "if image:\n",
    "    image_url = image.get('src', 'src 속성이 없습니다.')\n",
    "    print(image_url)\n",
    "else:\n",
    "    print(\"이미지를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 복사한 CSS 선택자로 이미지 태그를 찾기\n",
    "image = soup.select_one('#Col1-0-ThreeAmigos-Proxy > div > div:nth-child(1) > div.Pos\\(r\\).dustyImage.W\\(\\$ntkLeadWidth\\).Fl\\(start\\).article-cluster-boundary > a > div > img')\n",
    "\n",
    "# 이미지가 존재하면 src 속성을 출력\n",
    "if image:\n",
    "    image_url = image['src']\n",
    "    print(image_url)\n",
    "else:\n",
    "    print(\"이미지를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 특정 클래스와 함께 img 태그를 찾습니다.\n",
    "image = soup.find('img', alt=\"S&P: Medicare Advantage is new villain for hospitals\")\n",
    "\n",
    "if image:\n",
    "    image_url = image['src']\n",
    "    print(image_url)\n",
    "else:\n",
    "    print(\"이미지를 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s.yimg.com/uu/api/res/1.2/ytqLguc29AUWtIiL…4-08/f346d0b0-60bb-11ef-bffd-7cb8e497e511.cf.webp\n"
     ]
    }
   ],
   "source": [
    "image_url = \"https://s.yimg.com/uu/api/res/1.2/ytqLguc29AUWtIiL…4-08/f346d0b0-60bb-11ef-bffd-7cb8e497e511.cf.webp\"\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 웹 페이지의 HTML 가져오기\n",
    "url = \"https://finance.yahoo.com/\"  # 야후 파이낸스 메인 페이지\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# 2. BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 3. 모든 img 태그를 찾기\n",
    "images = soup.find_all('img')\n",
    "\n",
    "# 4. 이미지 저장 경로 설정\n",
    "save_dir = \"downloaded_images\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 5. 각 이미지 다운로드\n",
    "for idx, img in enumerate(images):\n",
    "    img_url = img.get('src')\n",
    "    \n",
    "    # img_url이 None이 아닌지 확인\n",
    "    if img_url:\n",
    "        # 확장자를 확인해서 없을 경우 기본값 추가 (jpg)\n",
    "        if not img_url.endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):\n",
    "            img_url += \".jpg\"\n",
    "\n",
    "        # 이미지 다운로드\n",
    "        try:\n",
    "            img_response = requests.get(img_url)\n",
    "            if img_response.status_code == 200:\n",
    "                img_data = img_response.content\n",
    "                img_filename = os.path.join(save_dir, f'image_{idx + 1}.jpg')\n",
    "                with open(img_filename, 'wb') as f:\n",
    "                    f.write(img_data)\n",
    "                print(f\"Downloaded: {img_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download: {img_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {img_url}: {e}\")\n",
    "    else:\n",
    "        print(\"No src found for an image tag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src 대신 data-src' 또는 'srcset 를 찾는다. ### 이미지 다운로드\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 웹 페이지의 HTML 가져오기\n",
    "url = \"https://edition.cnn.com/travel/caledonian-sleeper-london-scotland-night-train/index.html\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# 2. BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 3. 모든 img 태그를 찾기\n",
    "images = soup.find_all('img')\n",
    "\n",
    "# 4. 이미지 저장 경로 설정\n",
    "save_dir = \"downloaded_images\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 5. 각 이미지 다운로드\n",
    "for idx, img in enumerate(images):\n",
    "    # 'src' 대신 'data-src' 또는 'srcset'이 있는지 확인\n",
    "    img_url = img.get('data-src') or img.get('srcset') or img.get('src')\n",
    "    \n",
    "    if img_url:\n",
    "        # srcset의 경우 여러 해상도의 이미지가 콤마로 구분되어 있을 수 있음\n",
    "        if ',' in img_url:\n",
    "            img_url = img_url.split(',')[-1].strip().split(' ')[0]\n",
    "\n",
    "        # 이미지 확장자를 추출\n",
    "        img_ext = os.path.splitext(img_url)[1]  # URL에서 확장자를 추출\n",
    "        if not img_ext:\n",
    "            img_ext = '.jpg'  # 확장자가 없으면 기본값으로 .jpg 사용\n",
    "\n",
    "        # 이미지 다운로드\n",
    "        try:\n",
    "            img_response = requests.get(img_url)\n",
    "            if img_response.status_code == 200:\n",
    "                img_data = img_response.content\n",
    "                img_filename = os.path.join(save_dir, f'image_{idx + 1}{img_ext}')\n",
    "                with open(img_filename, 'wb') as f:\n",
    "                    f.write(img_data)\n",
    "                print(f\"Downloaded: {img_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download: {img_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {img_url}: {e}\")\n",
    "    else:\n",
    "        print(\"No src found for an image tag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipped invalid URL: h_100\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n",
      "Skipped invalid URL: c_fill\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 1. 웹 페이지의 HTML 가져오기\n",
    "url = \"https://edition.cnn.com/travel/caledonian-sleeper-london-scotland-night-train/index.html\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# 2. BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 3. 이미지와 동영상 태그를 찾기\n",
    "images = soup.find_all('img')\n",
    "videos = soup.find_all('video')\n",
    "\n",
    "# 4. 이미지 및 동영상 저장 경로 설정\n",
    "save_dir = \"downloaded_media\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 5. 이미지 다운로드\n",
    "for idx, img in enumerate(images):\n",
    "    # 'src' 대신 'data-src' 또는 'srcset'이 있는지 확인\n",
    "    img_url = img.get('data-src') or img.get('srcset') or img.get('src')\n",
    "    \n",
    "    if img_url:\n",
    "        # srcset의 경우 여러 해상도의 이미지가 콤마로 구분되어 있을 수 있음\n",
    "        if ',' in img_url:\n",
    "            img_url = img_url.split(',')[-1].strip().split(' ')[0]\n",
    "\n",
    "        # URL이 이미지 파일을 가리키는지 확인\n",
    "        if not img_url.lower().endswith(('.jpg', '.jpeg', '.png', '.gif', '.webp')):\n",
    "            print(f\"Skipped invalid URL: {img_url}\")\n",
    "            continue\n",
    "\n",
    "        # URL이 상대 경로인 경우 절대 경로로 변환\n",
    "        img_url = urljoin(url, img_url)\n",
    "\n",
    "        # 이미지 확장자를 추출\n",
    "        img_ext = os.path.splitext(img_url)[1]  # URL에서 확장자를 추출\n",
    "        if not img_ext:\n",
    "            img_ext = '.jpg'  # 확장자가 없으면 기본값으로 .jpg 사용\n",
    "\n",
    "        # 이미지 다운로드\n",
    "        try:\n",
    "            img_response = requests.get(img_url)\n",
    "            if img_response.status_code == 200:\n",
    "                img_data = img_response.content\n",
    "                img_filename = os.path.join(save_dir, f'image_{idx + 1}{img_ext}')\n",
    "                with open(img_filename, 'wb') as f:\n",
    "                    f.write(img_data)\n",
    "                print(f\"Downloaded: {img_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download: {img_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {img_url}: {e}\")\n",
    "    else:\n",
    "        print(\"No src found for an image tag\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 기사 제목을 먼저 찾습니다.\n",
    "headline = soup.find('h2', string=\"S&P: Medicare Advantage is new villain for hospitals\")\n",
    "\n",
    "# 해당 제목의 부모 컨테이너(예: div 또는 article)를 찾습니다.\n",
    "if headline:\n",
    "    parent_container = headline.find_parent()  # 부모 태그 찾기\n",
    "    \n",
    "    # 부모 컨테이너 안에서 이미지를 찾습니다.\n",
    "    image = parent_container.find('img')\n",
    "    \n",
    "    if image:\n",
    "        # 'src', 'data-src', 'srcset' 속성 순으로 확인하여 이미지 URL을 얻습니다.\n",
    "        image_url = image.get('data-src') or image.get('srcset') or image.get('src')\n",
    "        \n",
    "        # srcset 속성이 있는 경우, 가장 큰 해상도의 이미지 URL을 추출\n",
    "        if 'srcset' in image.attrs:\n",
    "            image_url = image['srcset'].split(',')[-1].strip().split(' ')[0]\n",
    "\n",
    "        print(image_url)\n",
    "    else:\n",
    "        print(\"이미지를 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(\"제목을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://s.yimg.com/uu/api/res/1.2/eMtesAlmzqE3evW6yvJZkw--~B/Zmk9c3RyaW07aD03Mzg7cT04MDt3PTEzMTI7YXBwaWQ9eXRhY2h5b24-/https://s.yimg.com/os/creatr-uploaded-images/2024-08/f346d0b0-60bb-11ef-bffd-7cb8e497e511.cf.webp\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 웹 페이지의 HTML 가져오기\n",
    "url = \"https://finance.yahoo.com/\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 클래스 이름을 사용하여 이미지 태그 찾기\n",
    "image = soup.find('img', class_='tw-bg-opacity-25 tw-rounded-md tw-w-full tw-aspect-[1.77] yf-13q9uv1')\n",
    "\n",
    "if image:\n",
    "    # 'src', 'data-src', 'srcset' 속성 순으로 확인하여 이미지 URL을 얻습니다.\n",
    "    image_url = image.get('data-src') or image.get('srcset') or image.get('src')\n",
    "    \n",
    "    # srcset 속성이 있는 경우, 가장 큰 해상도의 이미지 URL을 추출\n",
    "    if 'srcset' in image.attrs:\n",
    "        image_url = image['srcset'].split(',')[-1].strip().split(' ')[0]\n",
    "\n",
    "    print(image_url)\n",
    "else:\n",
    "    print(\"이미지를 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지를 찾을 수 없습니다.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 웹 페이지의 HTML 가져오기\n",
    "url = \"https://finance.yahoo.com/\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 1. 제목을 기준으로 관련 부모 요소 찾기\n",
    "headline = soup.find('h2', string=\"S&P: Medicare Advantage is new villain for hospitals\")\n",
    "\n",
    "if headline:\n",
    "    # 2. 부모 요소를 더 넓게 탐색\n",
    "    parent_container = headline.find_parent('div')  # 부모를 특정 'div'로 찾기\n",
    "    \n",
    "    if not parent_container:  # div가 없으면 더 상위 요소로 확장\n",
    "        parent_container = headline.find_parent()\n",
    "    \n",
    "    # 3. 부모 컨테이너 안에서 이미지 태그 찾기 (다양한 방법으로 시도)\n",
    "    image = None\n",
    "    if parent_container:\n",
    "        image = parent_container.find('img')\n",
    "    \n",
    "    if not image:  # 형제 요소 탐색\n",
    "        image = headline.find_next_sibling('img')\n",
    "    \n",
    "    if not image:  # 자식 요소에서 이미지 탐색\n",
    "        image = headline.find('img')\n",
    "    \n",
    "    if image:\n",
    "        # 4. 이미지 태그의 클래스 속성 추출\n",
    "        image_classes = image.get('class')\n",
    "        if image_classes:\n",
    "            image_classes_str = ' '.join(image_classes)  # 클래스 목록을 문자열로 결합\n",
    "            print(f\"이미지 태그의 클래스: {image_classes_str}\")\n",
    "        else:\n",
    "            print(\"이미지 태그에 클래스가 없습니다.\")\n",
    "        \n",
    "        # 5. 이미지 URL 추출 및 출력\n",
    "        image_url = image.get('data-src') or image.get('srcset') or image.get('src')\n",
    "        if 'srcset' in image.attrs:\n",
    "            image_url = image['srcset'].split(',')[-1].strip().split(' ')[0]\n",
    "        print(f\"이미지 URL: {image_url}\")\n",
    "    else:\n",
    "        print(\"이미지를 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(\"제목을 찾을 수 없습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://finance.yahoo.com/\"  # 야후 파이낸스 URL\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "# 전체 HTML을 텍스트 파일로 저장\n",
    "with open('page_structure.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이미지 또는 미디어가 제목 위에 있습니다.\n",
      "<div>\n",
      " <img alt=\"example image\" src=\"image.jpg\"/>\n",
      " <h2>\n",
      "  S&amp;P: Medicare Advantage is new villain for hospitals\n",
      " </h2>\n",
      " <p>\n",
      "  Some article content...\n",
      " </p>\n",
      "</div>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 예시 HTML\n",
    "html_content = \"\"\"\n",
    "<div>\n",
    "    <img src=\"image.jpg\" alt=\"example image\">\n",
    "    <h2>S&P: Medicare Advantage is new villain for hospitals</h2>\n",
    "    <p>Some article content...</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "# BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 1. 제목을 찾습니다\n",
    "headline = soup.find('h2', string=\"S&P: Medicare Advantage is new villain for hospitals\")\n",
    "\n",
    "if headline:\n",
    "    # 2. 제목의 이전 요소(이미지 또는 미디어)를 확인합니다\n",
    "    previous_sibling = headline.find_previous_sibling()\n",
    "    \n",
    "    # 3. 이전 요소가 이미지 또는 미디어인지 확인\n",
    "    if previous_sibling and (previous_sibling.name == 'img' or 'media' in previous_sibling.get('class', [])):\n",
    "        print(\"이미지 또는 미디어가 제목 위에 있습니다.\")\n",
    "    else:\n",
    "        # 4. 이미지나 미디어가 없으면, 해당 요소를 제거합니다\n",
    "        if previous_sibling:\n",
    "            previous_sibling.decompose()\n",
    "            print(\"이미지나 미디어가 없어서 제거했습니다.\")\n",
    "else:\n",
    "    print(\"제목을 찾을 수 없습니다.\")\n",
    "\n",
    "# 결과 HTML 출력\n",
    "print(soup.prettify())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불필요한 기사 콘텐츠와 특정 클래스 요소가 제거된 HTML이 'cleaned_page_structure.html' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "# 붎필요한 기사 콘텐츠와 특정 클래스 요소가 제거된 HTML 파일을 저장합니다.\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 저장된 HTML 파일 읽어오기\n",
    "with open('page_structure.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 1. 불필요한 기사 콘텐츠 삭제 (예: 특정 키워드 포함하는 <p> 또는 <div> 태그)\n",
    "keywords_to_remove = [\"Yahoo Finance\", \"Breaking News\", \"Sponsored\", \"Advertisement\"]\n",
    "\n",
    "# <p> 태그 내에서 특정 키워드가 포함된 요소 삭제\n",
    "for keyword in keywords_to_remove:\n",
    "    paragraphs = soup.find_all('p', string=lambda text: text and keyword in text)\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph.decompose()\n",
    "\n",
    "# <div> 태그 내에서 특정 키워드가 포함된 요소 삭제\n",
    "for keyword in keywords_to_remove:\n",
    "    divs = soup.find_all('div', string=lambda text: text and keyword in text)\n",
    "    for div in divs:\n",
    "        div.decompose()\n",
    "\n",
    "# 2. 특정 클래스 또는 ID를 가진 요소 삭제 (예: 광고나 비관련 요소)\n",
    "classes_to_remove = [\"ad\", \"sponsored-content\", \"related-articles\"]\n",
    "for class_name in classes_to_remove:\n",
    "    elements = soup.find_all(class_=class_name)\n",
    "    for element in elements:\n",
    "        element.decompose()\n",
    "\n",
    "# 3. 결과를 새 파일로 저장\n",
    "with open('cleaned_page_structure.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())\n",
    "\n",
    "print(\"불필요한 기사 콘텐츠와 특정 클래스 요소가 제거된 HTML이 'cleaned_page_structure.html' 파일에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 데이터가 제거된 HTML이 'cleaned_page_structure_no_json.html' 파일에 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 저장된 HTML 파일 읽어오기\n",
    "with open('page_structure.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 1. JSON 데이터를 포함하는 <script> 태그 삭제\n",
    "scripts = soup.find_all('script', type='application/json')\n",
    "for script in scripts:\n",
    "    script.decompose()\n",
    "\n",
    "# 2. <script> 태그 내에 있는 JSON 형식의 텍스트 삭제 (일반적인 script 태그)\n",
    "# 이 경우 JSON 데이터가 일반적인 <script> 태그 내에 포함되어 있을 수 있으므로 추가로 처리\n",
    "generic_scripts = soup.find_all('script')\n",
    "for script in generic_scripts:\n",
    "    # 스크립트의 내용이 JSON처럼 보이면 삭제\n",
    "    if script.string and script.string.strip().startswith('{') and script.string.strip().endswith('}'):\n",
    "        script.decompose()\n",
    "\n",
    "# 결과를 새 파일로 저장\n",
    "with open('cleaned_page_structure_no_json.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(soup.prettify())\n",
    "\n",
    "print(\"JSON 데이터가 제거된 HTML이 'cleaned_page_structure_no_json.html' 파일에 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 웹 페이지의 HTML 가져오기\n",
    "url = \"https://x.com/m1stra3/status/1780149198236905789\"\n",
    "response = requests.get(url)\n",
    "html_content = response.text\n",
    "\n",
    "# 2. BeautifulSoup을 사용하여 HTML 파싱\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# 3. 이미지와 동영상 태그를 찾기\n",
    "images = soup.find_all('img')\n",
    "videos = soup.find_all('video')\n",
    "\n",
    "# 4. 이미지 및 동영상 저장 경로 설정\n",
    "save_dir = \"downloaded_media\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# 5. 이미지 다운로드\n",
    "for idx, img in enumerate(images):\n",
    "    # 'src' 대신 'data-src' 또는 'srcset'이 있는지 확인\n",
    "    img_url = img.get('data-src') or img.get('srcset') or img.get('src')\n",
    "    \n",
    "    if img_url:\n",
    "        # srcset의 경우 여러 해상도의 이미지가 콤마로 구분되어 있을 수 있음\n",
    "        if ',' in img_url:\n",
    "            img_url = img_url.split(',')[-1].strip().split(' ')[0]\n",
    "\n",
    "        # 이미지 확장자를 추출\n",
    "        img_ext = os.path.splitext(img_url)[1]  # URL에서 확장자를 추출\n",
    "        if not img_ext:\n",
    "            img_ext = '.jpg'  # 확장자가 없으면 기본값으로 .jpg 사용\n",
    "\n",
    "        # 이미지 다운로드\n",
    "        try:\n",
    "            img_response = requests.get(img_url)\n",
    "            if img_response.status_code == 200:\n",
    "                img_data = img_response.content\n",
    "                img_filename = os.path.join(save_dir, f'image_{idx + 1}{img_ext}')\n",
    "                with open(img_filename, 'wb') as f:\n",
    "                    f.write(img_data)\n",
    "                print(f\"Downloaded: {img_filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download: {img_url}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {img_url}: {e}\")\n",
    "    else:\n",
    "        print(\"No src found for an image tag\")\n",
    "\n",
    "# 6. 동영상 다운로드\n",
    "for idx, video in enumerate(videos):\n",
    "    video_sources = video.find_all('source')\n",
    "    \n",
    "    for source in video_sources:\n",
    "        video_url = source.get('src') or source.get('data-src')\n",
    "        \n",
    "        if video_url:\n",
    "            # 동영상 확장자를 추출\n",
    "            video_ext = os.path.splitext(video_url)[1]\n",
    "            if not video_ext:\n",
    "                video_ext = '.mp4'  # 확장자가 없으면 기본값으로 .mp4 사용\n",
    "\n",
    "            # 동영상 다운로드\n",
    "            try:\n",
    "                video_response = requests.get(video_url)\n",
    "                if video_response.status_code == 200:\n",
    "                    video_data = video_response.content\n",
    "                    video_filename = os.path.join(save_dir, f'video_{idx + 1}{video_ext}')\n",
    "                    with open(video_filename, 'wb') as f:\n",
    "                        f.write(video_data)\n",
    "                    print(f\"Downloaded: {video_filename}\")\n",
    "                else:\n",
    "                    print(f\"Failed to download: {video_url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading {video_url}: {e}\")\n",
    "        else:\n",
    "            print(\"No src found for a video source\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html data-layout-uri=\"cms.cnn.com/_layouts/layout-with-rail/instances/travel-article-v1@published\" data-uri=\"cms.cnn.com/_pages/cm03l58up0000fzqqatha5xhd@published\" lang=\"en\">\n",
      " <head>\n",
      "  <style>\n",
      "   :root{--web-large-type-body-2-400-font-family:cnn_sans_display;--web-large-type-body-2-400-font-size:16px;--web-large-type-body-2-400-font-style:Regular;--web-large-type-body-2-400-line-height:28px;--web-large-type-body-2-400-letter-spacing:0px;--web-large-type-body-2-400-text-decoration:NONE;--web-large-type-body-2-400-font-weight:400;--web-large-type-other-timestamp-400-font-family:cnn_sans_display;--web-large-type-other-timestamp-400-font-size:14px;--web-large-type-other-timestamp-400-font-style:Regular;--web-large-type-other-timestamp-400-line-height:20px;--web-large-type-other-timestamp-400-letter-spacing:0%;--web-large-type-other-timestamp-400-text-decoration:NONE;--web-large-type-other-timestamp-400-font-weight:400;--style-type-primary-4-low:#6e6e6eff;--style-type-prim\n",
      "\n",
      "페이지 제목:\n",
      "The sleeper train that’s a wild ride to another world | CNN\n",
      "\n",
      "기사 목록:\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "# 웹 페이지 가져오기\n",
    "response = requests.get('https://news.ycombinator.com/news')\n",
    "\n",
    "# HTML 파싱\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# 1. HTML 전체를 prettify()로 보기 좋게 출력\n",
    "pretty_html = soup.prettify()\n",
    "print(pretty_html[:1000])  # 처음 1000자만 출력\n",
    "\n",
    "# 2. HTML의 일부만 출력 (예: 페이지의 제목만 출력)\n",
    "print(\"\\n페이지 제목:\")\n",
    "print(soup.title.string)\n",
    "\n",
    "# 3. 특정 요소만 추출해서 보기 좋게 출력 (예: 기사 목록)\n",
    "print(\"\\n기사 목록:\")\n",
    "articles = soup.find_all('a', class_='storylink')\n",
    "for idx, article in enumerate(articles[:5], start=1):  # 처음 5개의 기사만 출력\n",
    "    print(f\"{idx}. {article.text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
