{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "私はダッシーヌーズでプールで1回撮ったことがあるんですけど その時も実は全部入りたいと言ってたんですけどその時はできなくて 今回多分全部入れるんじゃないかなということで 水中撮影ができるんじゃないかなということで楽しみにしています やっぱりカメラにすっかり あ、やっぱりカメラにすっかり\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "audio_file= open(\"E:\\\\onedrive\\\\Documents\\\\vs code\\\\audio\\\\mina.wav\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What's your favorite scene? I shot a scene in the pool once. At that time, I said I wanted to shoot everything, but I couldn't do it. This time, I think I can shoot everything. I'm looking forward to shooting underwater. I'm looking forward to it. Open your eyes. Open your eyes. Okay. What's this? It's shaking in the water. Okay. I'm scared.\n"
     ]
    }
   ],
   "source": [
    "# 영어로 자동 번역\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "audio_file= open(\"E:\\\\onedrive\\\\Documents\\\\vs code\\\\audio\\\\mina.wav\", \"rb\")\n",
    "translation = client.audio.translations.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(translation.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat gpt 번역"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "저는 예전에 풀장에서 한 번 촬영해본 적이 있는데 그때도 사실 전부 물에 들어가고 싶었지만 그때는 할 수 없었어요. 이번에는 아마 전부 물에 들어갈 수 있을 것 같아서 수중 촬영을 할 수 있을 것 같아 기대하고 있어요.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "# OpenAI 클라이언트 설정\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "audio_file= open(\"E:\\\\onedrive\\\\Documents\\\\vs code\\\\audio\\\\mina.wav\", \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "\n",
    "japanese_text = transcription.text\n",
    "\n",
    "# Chat API를 사용하여 일본어 텍스트를 한국어로 번역\n",
    "\n",
    "context = \"트와이스 미나의 뮤직비디오 비하인드 영상입니다. 물에 들어가서 촬영하는 장면임\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a translator.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Translate the following Japanese text to Korean considering the context '{context}': {japanese_text}\"},\n",
    "    ]\n",
    ")\n",
    "korean_text = response.choices[0].message.content\n",
    "\n",
    "print(korean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os\n",
    "\n",
    "# Whisper 모델 로드\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# 파일들이 있는 디렉토리 경로\n",
    "directory = r\"C:\\Users\\ilove\\Videos\\output\"\n",
    "\n",
    "# 모든 분할 파일을 순회하며 처리\n",
    "for i in range(11):  # 0부터 10까지 순회 (11개 파일)\n",
    "    file_name = f\"output_{i:03}.mp3\"  # output_000.mp3, output_001.mp3 등\n",
    "    file_path = os.path.join(directory, file_name)\n",
    "    \n",
    "    # 오디오 파일을 로드하고 패딩/트림 처리\n",
    "    audio = whisper.load_audio(file_path)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "    \n",
    "    # log-Mel 스펙트로그램 생성 및 모델로 이동\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "    \n",
    "    # 언어 감지\n",
    "    _, probs = model.detect_language(mel)\n",
    "    print(f\"File: {file_name}, Detected language: {max(probs, key=probs.get)}\")\n",
    "    \n",
    "    # 오디오 디코딩\n",
    "    options = whisper.DecodingOptions()\n",
    "    result = whisper.decode(model, mel, options)\n",
    "    \n",
    "    # 전사된 텍스트 출력\n",
    "    print(f\"Transcript for {file_name}:\\n{result.text}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
