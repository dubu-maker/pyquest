{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./image2.png\" alt=\"Neural Network\" style=\"width: 50%; max-width: 100%; display: block; margin: 0 auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n0은 입력층에 해당, 입력 특징의 수를 나타낸다. n[0] = 4\n",
    "n[1], n[2], n[3]은 숨겨진 층(히든 레이어)들의 뉴런수를 나타낸다. \n",
    "첫번째 층 n[1]은 4\n",
    "두번째 층 n[2]은 3\n",
    "세번째 층 n[3]은 2\n",
    "n[4]는 출력층의 뉴런 수, 그림에서 출력층은 한개 이므로 n[4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망의 역전파과정에서 출력층에 대한 미분 값인 dZ[l]\n",
    "dZ[l]은 예측값 A[L]과 실제 값 Y의 차이로 구한다\n",
    "$$\n",
    "dZ[L]=A[L]−Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 데이터 (m, n_x)\n",
    "# m은 데이터의 샘플 수\n",
    "# n_x는 데이터 포인트의 특징 수\n",
    "layer_dims = [n_x, 4, 3, 2, 1] # n_x는 입력 데이터의 특징 수(차원 수)\n",
    "parameters = {}\n",
    "\n",
    "for i in range(1, len(layer_dims)):\n",
    "    parameters['W' + str(i)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
    "    parameters['b' + str(i)] = np.zeros((layer_dims[i], 1))\n",
    "# 첫 번째 레이어에는 n_x개의 뉴런을 가진다\n",
    "# 두 번째 레이어(숨겨진) 에는 4개의 뉴런\n",
    "# 세 번째 레이어는 3개의 뉴런\n",
    "# 네 번째 레이어는 2개의 뉴런\n",
    "# 마지막 출력층 레이어는 1개의 뉴런을 가진다.\n",
    "\n",
    "# 매개 변수 초기화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설명하자면:\n",
    "\n",
    "### W[1] (첫 번째 가중치 행렬):\n",
    "- **형태**: (첫 번째 숨겨진 층의 뉴런 수, 입력 데이터의 피처 수)\n",
    "- 첫 번째 숨겨진 층의 뉴런 수는 3개, 입력 데이터의 피처 수는 4개입니다.\n",
    "- 따라서 W[1]의 크기는 (3, 4)입니다.\n",
    "\n",
    "### W[2] (두 번째 가중치 행렬):\n",
    "- **형태**: (출력층의 뉴런 수, 첫 번째 숨겨진 층의 뉴런 수)\n",
    "- 출력층의 뉴런 수는 1개, 첫 번째 숨겨진 층의 뉴런 수는 3개입니다.\n",
    "- 따라서 W[2]의 크기는 (1, 3)입니다.\n",
    "\n",
    "#### 혼동을 줄 수 있는 부분: 이미지에서 두 번째 레이어에 해당하는 뉴런들 \\(a^{[2]}\\)가 4개인 것처럼 보일 수 있지만, 이는 레이어가 아니라 첫 번째 숨겨진 층에 해당하는 뉴런들입니다. 즉, 첫 번째 숨겨진 층에는 3개의 뉴런이 있고, 출력층에 1개의 뉴런이 있습니다.\n",
    "\n",
    "### b[1] (첫 번째 편향 벡터):\n",
    "- **형태**: (첫 번째 숨겨진 층의 뉴런 수, 1)\n",
    "- 첫 번째 숨겨진 층에 3개의 뉴런이 있으므로, b[1]의 크기는 (3, 1)입니다.\n",
    "\n",
    "### b[2] (두 번째 편향 벡터):\n",
    "- **형태**: (출력층의 뉴런 수, 1)\n",
    "- 출력층에는 1개의 뉴런이 있으므로, b[2]의 크기는 (1, 1)입니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 요약:\n",
    "- **W[1]**: (3, 4) → 첫 번째 숨겨진 층의 3개의 뉴런이 입력 데이터의 4개 피처와 연결됩니다.\n",
    "- **W[2]**: (1, 3) → 출력층의 1개의 뉴런이 첫 번째 숨겨진 층의 3개의 뉴런과 연결됩니다.\n",
    "- **b[1]**: (3, 1) → 첫 번째 숨겨진 층의 3개 뉴런에 대해 하나의 편향 값이 있습니다.\n",
    "- **b[2]**: (1, 1) → 출력층의 1개 뉴런에 대해 하나의 편향 값이 있습니다. \n",
    "\n",
    "이런 식으로 각 가중치와 편향의 형태를 결정합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### W[1]의 3: 첫 번째 숨겨진 층의 뉴런들이 입력층과 연결됩니다.\n",
    "### W[2]의 3: 첫 번째 숨겨진 층의 같은 뉴런들이 출력층과 연결됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "표로 정리해보면 구조를 쉽게 이해할 수 있을 것 같아요. 첫 번째 숨겨진 층의 뉴런들과 가중치 행렬의 연결을 설명하는 표를 만들어 보겠습니다.\n",
    "\n",
    "| 층      | 가중치 행렬(W) | 가중치 행렬 크기  | 설명                                              |\n",
    "|---------|----------------|------------------|---------------------------------------------------|\n",
    "| 입력층   | -              | (4, 1)           | 입력 데이터(X)에서 네 가지 특징 (x1, x2, x3, x4) |\n",
    "| 첫 번째 숨겨진 층 | **W[1]**          | (3, 4)          | 첫 번째 숨겨진 층의 3개 뉴런들이 입력층의 4개 뉴런과 연결됨  |\n",
    "| 첫 번째 숨겨진 층 | **b[1]**          | (3, 1)          | 첫 번째 숨겨진 층의 3개 뉴런에 대한 편향           |\n",
    "| 출력층   | **W[2]**          | (1, 3)          | 출력층 뉴런이 첫 번째 숨겨진 층의 3개 뉴런과 연결됨 |\n",
    "| 출력층   | **b[2]**          | (1, 1)          | 출력층 뉴런에 대한 편향                          |\n",
    "\n",
    "### 표 설명:\n",
    "1. **입력층**: 4개의 입력 뉴런(x1, x2, x3, x4)로 구성되어 있습니다. 이 입력 값들은 첫 번째 숨겨진 층으로 전달됩니다.\n",
    "2. **첫 번째 숨겨진 층**: 여기에는 3개의 뉴런이 있으며, 입력층의 4개의 입력 값과 연결됩니다. 이 연결을 나타내는 가중치 행렬이 **W[1]**입니다. **W[1]**의 크기는 (3, 4)입니다. 이 가중치는 첫 번째 숨겨진 층의 뉴런들이 입력 뉴런들과 어떻게 연결되는지를 결정합니다.\n",
    "3. **출력층**: 출력층에는 1개의 뉴런이 있으며, 첫 번째 숨겨진 층의 3개 뉴런과 연결됩니다. 이 연결을 나타내는 가중치 행렬이 **W[2]**입니다. **W[2]**의 크기는 (1, 3)으로, 출력 뉴런이 첫 번째 숨겨진 층의 3개 뉴런들과 연결됩니다.\n",
    "\n",
    "이렇게 표로 정리하면 각 층의 가중치 행렬들이 어떻게 연결되는지 잘 보일 것입니다.\n",
    "\n",
    "이 표를 바탕으로, 신경망의 구조를 그림으로 그리려면 각 층의 뉴런들이 서로 어떻게 연결되는지에 따라 선을 그리면 되는데, 그림을 그리는 것보다는 표로 보는 것이 직관적일 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "신경망의 구조에서 `파라미터(parameters)`는 가중치(`W`)와 편향(`b`)을 포함하며, 각 층 사이의 연결과 각 뉴런에 대한 정보를 저장합니다. `cache`는 순전파 과정에서 계산된 중간 값들(활성화 값, 선형 연산 값 등)을 저장하여 역전파에서 사용됩니다. `grads`는 역전파 과정에서 구해진 각 파라미터(W, b)에 대한 미분 값(경사도)입니다.\n",
    "\n",
    "이 구조를 설명하기 위해 파라미터, 캐시, 그리고 기울기가 어떻게 사용되고 업데이트되는지 간단히 정리할 수 있습니다.\n",
    "\n",
    "### 1. **`파라미터(parameters)`의 위치**:\n",
    "- **W**: 각 층 사이의 가중치 행렬입니다. 예를 들어, `W[1]`은 입력층(Layer 1)과 첫 번째 은닉층(Layer 2)을 연결하는 가중치입니다. 마찬가지로, `W[2]`는 첫 번째 은닉층(Layer 2)과 출력층(Layer 3)을 연결하는 가중치입니다.\n",
    "- **b**: 각 층에 해당하는 편향 값입니다. 예를 들어, `b[1]`은 첫 번째 은닉층에 적용되는 편향입니다. 각 뉴런에 대해 하나의 편향 값이 존재합니다.\n",
    "\n",
    "### 2. **`캐시(cache)`의 위치**:\n",
    "- 캐시는 순전파에서 계산된 중간 값들이 저장됩니다. 예를 들어, 각 층에서 계산된 활성화 값(`A`)과 선형 연산의 출력 값(`Z`) 등이 저장됩니다.\n",
    "- 캐시의 주 목적은 역전파 과정에서 중간 값을 다시 계산하지 않고도 바로 사용할 수 있도록 저장하는 것입니다.\n",
    "\n",
    "### 3. **`기울기(grads)`의 위치**:\n",
    "- 기울기(`grads`)는 역전파 과정에서 구해진 가중치와 편향에 대한 미분 값입니다. 이 값들은 경사 하강법을 통해 가중치와 편향을 업데이트하는 데 사용됩니다.\n",
    "- 예를 들어, `dW[1]`은 가중치 `W[1]`에 대한 기울기이며, `db[1]`은 편향 `b[1]`에 대한 기울기입니다.\n",
    "\n",
    "### 파라미터 업데이트 과정\n",
    "파라미터는 기울기를 이용해 경사 하강법(Gradient Descent)으로 업데이트됩니다. 이때 파라미터가 어떻게 변하는지 보여드리면:\n",
    "\n",
    "```python\n",
    "W[1] = W[1] - learning_rate * dW[1]\n",
    "b[1] = b[1] - learning_rate * db[1]\n",
    "W[2] = W[2] - learning_rate * dW[2]\n",
    "b[2] = b[2] - learning_rate * db[2]\n",
    "```\n",
    "\n",
    "즉, 기울기(`dW[1]`, `db[1]`, `dW[2]`, `db[2]`)를 사용하여 가중치(`W`)와 편향(`b`)를 조금씩 조정하면서 손실을 줄이는 방향으로 파라미터가 업데이트됩니다.\n",
    "\n",
    "### 신경망을 구성하는 딕셔너리의 구조 예시:\n",
    "```python\n",
    "parameters = {\n",
    "    \"W1\": W[1],  # W1: 입력층 -> 은닉층으로 가는 가중치\n",
    "    \"b1\": b[1],  # b1: 은닉층의 편향\n",
    "    \"W2\": W[2],  # W2: 은닉층 -> 출력층으로 가는 가중치\n",
    "    \"b2\": b[2],  # b2: 출력층의 편향\n",
    "}\n",
    "\n",
    "cache = {\n",
    "    \"Z1\": Z[1],  # 은닉층에서의 선형 연산 결과\n",
    "    \"A1\": A[1],  # 은닉층에서의 활성화 값\n",
    "    \"Z2\": Z[2],  # 출력층에서의 선형 연산 결과\n",
    "    \"A2\": A[2],  # 출력층에서의 활성화 값\n",
    "}\n",
    "\n",
    "grads = {\n",
    "    \"dW1\": dW[1],  # W1에 대한 기울기\n",
    "    \"db1\": db[1],  # b1에 대한 기울기\n",
    "    \"dW2\": dW[2],  # W2에 대한 기울기\n",
    "    \"db2\": db[2],  # b2에 대한 기울기\n",
    "}\n",
    "```\n",
    "\n",
    "### 그림에서 파라미터와 캐시, 기울기의 위치:\n",
    "- **`W`와 `b`**: 각 층 사이의 연결선에 `W`가 있고, 각 뉴런에 대한 추가 값으로 `b`가 있습니다.\n",
    "- **`cache`**: 그림에서 선형 연산의 중간 값(`Z`)이나 활성화 값(`A`)은 캐시에 저장되며, 이는 역전파에서 사용됩니다.\n",
    "- **`grads`**: 그림의 각 가중치와 편향에 대한 미분 값이 `grads`에 저장되고, 이를 이용해 파라미터가 업데이트됩니다.\n",
    "\n",
    "이런 방식으로 신경망의 학습이 이루어집니다. 만약 이 과정에서 추가적인 구체적 예나 코드 설명이 필요하다면, 더 자세히 설명드릴 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Outline 설명:\n",
    "Initialize Parameters (매개변수 초기화):\n",
    "\n",
    "신경망을 만들기 전에, 각 레이어의 가중치 \n",
    "𝑊\n",
    "W와 바이어스 \n",
    "𝑏\n",
    "b를 초기화해야 합니다. 2층 신경망뿐만 아니라, \n",
    "𝐿\n",
    "L 층의 신경망까지 지원하는 초기화 과정을 거칩니다.\n",
    "\n",
    "\n",
    "Forward Propagation Module (순전파 모듈):\n",
    "\n",
    "순전파 단계에서는 입력 데이터를 레이어를 통해 전달하면서 신경망이 예측값을 출력합니다. 여기서 각 레이어는 선형 변환(Linear)과 활성화 함수(Activation)를 사용합니다.\n",
    "그림에서 보라색으로 표시된 부분이 순전파 과정을 나타내며, 각 레이어에서는 선형 변환 후 활성화 함수(ReLU 혹은 Sigmoid)를 적용합니다.\n",
    "\n",
    "\n",
    "Backpropagation Module (역전파 모듈):\n",
    "\n",
    "손실을 구한 후, 역전파를 통해 각 가중치와 바이어스에 대한 미분 값을 계산하여 기울기를 구하고, 이를 통해 매개변수(가중치와 바이어스)를 업데이트합니다.\n",
    "그림에서 빨간색으로 표시된 부분이 역전파를 나타내며, 순전파에서 저장된 값(cache)을 사용하여 기울기를 계산하고, 이를 매개변수 업데이트에 활용합니다.\n",
    "\n",
    "\n",
    "Cache 사용:\n",
    "\n",
    "각 순전파 단계에서 계산된 값들은 캐시에 저장됩니다. 이는 역전파 단계에서 해당 값들을 다시 사용하여 기울기를 계산하는 데 활용되기 때문에 매우 중요합니다.\n",
    "\n",
    "\n",
    "매개변수 업데이트:\n",
    "\n",
    "계산된 기울기를 바탕으로 매개변수 \n",
    "𝑊\n",
    "W와 \n",
    "𝑏\n",
    "b를 업데이트하는 단계입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "모델의 구조는 다음과 같습니다: LINEAR -> RELU -> LINEAR -> SIGMOID. <br>\n",
    "가중치 행렬을 위한 무작위 초기화는 다음과 같이 사용하세요: np.random.randn(d0, d1, ..., dn) * 0.01 (올바른 형태로). np.random.randn에 대한 문서를 참조하세요. <br>\n",
    "바이어스를 위한 초기화는 0으로 합니다: np.zeros(shape) (올바른 형태로). np.zeros에 대한 문서를 참조하세요.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.zeros(n_h, 1)\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01    \n",
    "    b2 = np.zeros(n_y, 1)\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`layer_dims`는 각 레이어의 뉴런 수를 저장하는 **리스트**입니다. 신경망을 구성하는 각 층의 뉴런 개수를 나열한 구조입니다.\n",
    "\n",
    "### 예시:\n",
    "\n",
    "만약 `layer_dims = [4, 3, 2, 1]`이라면, 이는 다음과 같은 신경망 구조를 나타냅니다:\n",
    "- **입력층**: 4개의 뉴런\n",
    "- **첫 번째 숨겨진 층**: 3개의 뉴런\n",
    "- **두 번째 숨겨진 층**: 2개의 뉴런\n",
    "- **출력층**: 1개의 뉴런\n",
    "\n",
    "이 리스트는 **각 레이어의 뉴런 개수를 나열한 것**입니다.\n",
    "\n",
    "### 리스트 요소:\n",
    "\n",
    "- `layer_dims[0]`: 입력층의 뉴런 수 (4개)\n",
    "- `layer_dims[1]`: 첫 번째 숨겨진 층의 뉴런 수 (3개)\n",
    "- `layer_dims[2]`: 두 번째 숨겨진 층의 뉴런 수 (2개)\n",
    "- `layer_dims[3]`: 출력층의 뉴런 수 (1개)\n",
    "\n",
    "### 왜 `layer_dims[l]`와 `layer_dims[l-1]`를 사용하는가?\n",
    "\n",
    "1. **가중치(W)의 크기**:\n",
    "   - 각 레이어의 가중치 행렬 `W[l]`은 `layer_dims[l]` × `layer_dims[l-1]` 크기를 가져야 합니다.\n",
    "   - 여기서 `l`은 현재 층의 번호를 나타냅니다.\n",
    "   \n",
    "   - **왜 `layer_dims[l]` × `layer_dims[l-1]`?**\n",
    "     - `layer_dims[l]`은 현재 층의 뉴런 수를 나타내고, \n",
    "     - `layer_dims[l-1]`은 이전 층의 뉴런 수를 나타냅니다.\n",
    "     - **가중치 행렬의 크기**는 **(현재 층의 뉴런 수, 이전 층의 뉴런 수)**로 구성됩니다.\n",
    "   \n",
    "   예를 들어, 첫 번째 숨겨진 층에서:\n",
    "   - `W[1]`의 크기는 `(layer_dims[1], layer_dims[0])` → (3, 4)입니다. 이는 첫 번째 숨겨진 층에 있는 3개의 뉴런이 입력층의 4개의 뉴런과 연결되어 있음을 의미합니다.\n",
    "   \n",
    "2. **바이어스(b)의 크기**:\n",
    "   - 각 레이어의 바이어스 벡터 `b[l]`은 `(layer_dims[l], 1)`의 크기를 가집니다.\n",
    "   - 바이어스는 각 뉴런에 하나씩 대응되므로, 바이어스 벡터의 크기는 해당 층의 뉴런 개수와 동일한 형태를 갖습니다.\n",
    "   \n",
    "   예를 들어, 첫 번째 숨겨진 층의 바이어스 `b[1]`의 크기는 `(layer_dims[1], 1)` → (3, 1)입니다. 이는 첫 번째 숨겨진 층에 3개의 뉴런이 있으므로, 각 뉴런마다 하나의 바이어스 값이 필요하다는 것을 나타냅니다.\n",
    "\n",
    "### 코드에서 `layer_dims[1]`과 `layer_dims[1-1]`의 의미:\n",
    "\n",
    "- `layer_dims[1]`: 첫 번째 숨겨진 층의 뉴런 수\n",
    "- `layer_dims[1-1] = layer_dims[0]`: 입력층의 뉴런 수\n",
    "\n",
    "이 값들은 가중치 행렬의 크기를 설정하는 데 사용됩니다.\n",
    "\n",
    "### 예시 코드 설명:\n",
    "\n",
    "```python\n",
    "parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "```\n",
    "\n",
    "이 코드는 가중치 행렬 `W[l]`을 난수로 초기화합니다. 여기서:\n",
    "\n",
    "- `layer_dims[l]`: 현재 레이어의 뉴런 수\n",
    "- `layer_dims[l-1]`: 이전 레이어의 뉴런 수\n",
    "\n",
    "따라서 가중치 행렬 `W[l]`의 크기는 **(현재 층의 뉴런 수, 이전 층의 뉴런 수)**입니다.\n",
    "\n",
    "### 예시로 본다면:\n",
    "- `l = 1`일 때, `W[1]`의 크기는 `(layer_dims[1], layer_dims[0])` → `(3, 4)`\n",
    "- `l = 2`일 때, `W[2]`의 크기는 `(layer_dims[2], layer_dims[1])` → `(2, 3)`\n",
    "\n",
    "즉, 각 층에서 이전 층과 연결된 가중치의 크기를 설정하는 것이 이 과정의 목적입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 내용을 업데이트하여 각 가중치(W)와 편향(b)의 형태를 설명하고 정리했습니다.\n",
    "\n",
    "### W[1] (첫 번째 가중치 행렬):\n",
    "- **형태**: (첫 번째 숨겨진 층의 뉴런 수, 입력 데이터의 피처 수)\n",
    "- 첫 번째 숨겨진 층의 뉴런 수는 3개, 입력 데이터의 피처 수는 4개입니다.\n",
    "- 따라서 W[1]의 크기는 **(3, 4)**입니다.\n",
    "\n",
    "### W[2] (두 번째 가중치 행렬):\n",
    "- **형태**: (두 번째 숨겨진 층의 뉴런 수, 첫 번째 숨겨진 층의 뉴런 수)\n",
    "- 두 번째 숨겨진 층의 뉴런 수는 2개, 첫 번째 숨겨진 층의 뉴런 수는 3개입니다.\n",
    "- 따라서 W[2]의 크기는 **(2, 3)**입니다.\n",
    "\n",
    "### W[3] (세 번째 가중치 행렬):\n",
    "- **형태**: (출력층의 뉴런 수, 두 번째 숨겨진 층의 뉴런 수)\n",
    "- 출력층의 뉴런 수는 1개, 두 번째 숨겨진 층의 뉴런 수는 2개입니다.\n",
    "- 따라서 W[3]의 크기는 **(1, 2)**입니다.\n",
    "\n",
    "#### 혼동을 줄 수 있는 부분:\n",
    "- 첫 번째 숨겨진 층의 뉴런이 3개, 두 번째 숨겨진 층의 뉴런이 2개, 출력층의 뉴런은 1개입니다.\n",
    "- 이미지에서 두 번째 레이어에 해당하는 뉴런들이 많아 보일 수 있지만, 각 층마다 설정된 뉴런 수를 기준으로 가중치가 결정됩니다.\n",
    "\n",
    "---\n",
    "\n",
    "### b[1] (첫 번째 편향 벡터):\n",
    "- **형태**: (첫 번째 숨겨진 층의 뉴런 수, 1)\n",
    "- 첫 번째 숨겨진 층에 3개의 뉴런이 있으므로, b[1]의 크기는 **(3, 1)**입니다.\n",
    "\n",
    "### b[2] (두 번째 편향 벡터):\n",
    "- **형태**: (두 번째 숨겨진 층의 뉴런 수, 1)\n",
    "- 두 번째 숨겨진 층에 2개의 뉴런이 있으므로, b[2]의 크기는 **(2, 1)**입니다.\n",
    "\n",
    "### b[3] (세 번째 편향 벡터):\n",
    "- **형태**: (출력층의 뉴런 수, 1)\n",
    "- 출력층에 1개의 뉴런이 있으므로, b[3]의 크기는 **(1, 1)**입니다.\n",
    "\n",
    "---\n",
    "\n",
    "#### 최종 요약:\n",
    "\n",
    "- **W[1]**: **(3, 4)** → 첫 번째 숨겨진 층의 3개의 뉴런이 입력 데이터의 4개 피처와 연결됩니다.\n",
    "- **W[2]**: **(2, 3)** → 두 번째 숨겨진 층의 2개의 뉴런이 첫 번째 숨겨진 층의 3개의 뉴런과 연결됩니다.\n",
    "- **W[3]**: **(1, 2)** → 출력층의 1개의 뉴런이 두 번째 숨겨진 층의 2개의 뉴런과 연결됩니다.\n",
    "- **b[1]**: **(3, 1)** → 첫 번째 숨겨진 층의 3개 뉴런에 대해 하나의 편향 값이 있습니다.\n",
    "- **b[2]**: **(2, 1)** → 두 번째 숨겨진 층의 2개 뉴런에 대해 하나의 편향 값이 있습니다.\n",
    "- **b[3]**: **(1, 1)** → 출력층의 1개 뉴런에 대해 하나의 편향 값이 있습니다.\n",
    "\n",
    "이렇게 각 가중치와 편향의 형태를 레이어 구조에 맞게 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        #(≈ 2 lines of code)\n",
    "        # parameters['W' + str(l)] = ...\n",
    "        # parameters['b' + str(l)] = ...\n",
    "        # YOUR CODE STARTS HERE\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01 # layer_dims[1] 현재층의 뉴런수, layer_dims[1 - 1] 이전 층, 즉 입력층의 뉴런수\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        \n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 - 전방 전파 모듈 <br>\n",
    "###4.1 - 선형 전방 전파 (Linear Forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    인수(Arguments):\n",
    "    A -- 이전 레이어에서의 활성화 값 (또는 입력 데이터): (이전 레이어의 크기, 예제의 개수)\n",
    "    W -- 가중치 행렬: 현재 레이어의 크기와 이전 레이어의 크기를 가진 넘파이 배열\n",
    "    b -- 편향 벡터: 현재 레이어의 크기와 1을 가진 넘파이 배열\n",
    "\n",
    "    반환(Returns):\n",
    "    Z -- 활성화 함수의 입력값, 또한 사전 활성화 파라미터라고 불림\n",
    "    cache -- \"A\", \"W\", \"b\"를 포함한 파이썬 튜플; 역전파를 효율적으로 계산하기 위해 저장됨\n",
    "    \"\"\"\n",
    "    \n",
    "    #(≈ 1 line of code)\n",
    "    # Z = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    Z = np.dot(W, A) + b # A는 이전 레이어의 출력 또는 입력 데이터\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear_forward() 함수는 전방 전파 과정에서 선형 계산을 수행하는 함수\n",
    "$Z = W \\cdot A_{prev} + b$\n",
    "\n",
    "- W는 가중치 행렬\n",
    "- A prev는 이전 레이어에서 나온 활성화 값 또는 입력 데이터\n",
    "- b는 편향 벡터\n",
    "- Z는 활성화 함수에 입력될 선형 계산의 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear_cache는 역전파 계산 시 필요한 값들을 저장하기 위해 사용되는 캐시입니다.<br>\n",
    " 이 캐시에는 전방 전파 과정에서 사용된 변수들인 Aprev,W,bA_{prev}, W, bAprev ,W,b가 저장됨 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "linear_forward는 신경망에서 한 레이어의 선형 부분만을 계산합니다. 이때 활성화 함수는 사용하지 않습니다. <br>\n",
    "linear_activation_forward는 한 레이어의 선형 계산과 활성화 함수를 결합하여 전방 전파를 수행합니다. <br>\n",
    "L_model_forward는 전체 신경망의 전방 전파를 처리하며, 각 레이어에서 linear_activation_forward를 호출하여 차례대로 계산을 수행하고, <br>\n",
    "마지막 레이어에서는 Sigmoid 활성화를 사용합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구체적인 과정:\n",
    "linear_activation_forward() 함수는 한 레이어에서 Z, A, W, b와 같은 값을 계산합니다.\n",
    "\n",
    "\n",
    "이 값들을 **cache**라는 변수에 저장하고, 이 cache를 나중에 역전파(backpropagation)에서 사용하기 위해 caches 리스트에 추가합니다.\n",
    "\n",
    "\n",
    "이렇게 모든 레이어에서의 cache들을 caches에 순차적으로 저장해 두면, 역전파 시 각 레이어의 cache 값을 쉽게 불러와서 역전파를 진행할 수 있습니다.\n",
    "\n",
    "\n",
    "정리:\n",
    "cache는 한 레이어에서 계산된 중간 값들을 저장한 튜플입니다.\n",
    "\n",
    "\n",
    "caches는 모든 레이어에서의 cache들을 모아놓은 리스트입니다.\n",
    "\n",
    "\n",
    "역전파 단계에서 기울기(gradient) 를 계산하기 위해 순전파 단계에서 계산된 중간 값들이 필요하므로, 이를 cache에 저장하고 caches 리스트에 추가합니다.\n",
    "\n",
    "\n",
    "따라서 caches.append(cache)는 역전파를 위한 준비 단계라고 생각하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = -1/m * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "# 손실 함수 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네, 전방전파 → 손실 함수 계산 → 역전파가 **딥러닝에서 학습의 일반적인 순서**입니다. 각각의 과정을 한 줄로 요약하면:\n",
    "\n",
    "1. **전방전파 (Forward Propagation)**: \n",
    "   - **입력 데이터를 사용하여 각 층의 가중치와 편향을 거쳐 예측값을 계산**합니다. 활성화 함수(ReLU, Sigmoid 등)를 통해 중간 값을 계산한 후, 최종 출력(예측값)을 얻습니다.\n",
    "   \n",
    "2. **손실 함수 계산 (Loss Function Calculation)**:\n",
    "   - **예측값과 실제 레이블(정답) 간의 차이를 계산**하여 모델의 성능을 평가합니다. 보통 교차 엔트로피 또는 MSE(평균 제곱 오차) 등의 손실 함수가 사용됩니다.\n",
    "\n",
    "3. **역전파 (Backward Propagation)**:\n",
    "   - **손실을 기반으로 각 가중치와 편향의 변화량(기울기)을 계산**합니다. 이 과정에서 기울기를 사용해 가중치와 편향을 업데이트하여 모델을 학습시킵니다.\n",
    "\n",
    "### 간단한 예시로 요약:\n",
    "- **전방전파**: 입력 → 예측값 계산\n",
    "- **손실 함수 계산**: 예측값 vs 실제값 → 오류 측정\n",
    "- **역전파**: 오류를 기반으로 가중치와 편향 업데이트\n",
    "\n",
    "이 과정을 반복하면서 신경망은 예측 오류를 줄이고 더 나은 성능을 발휘하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네, 정확히 맞습니다! **경사 하강법(Gradient Descent)**은 신경망에서 **오류를 줄이기 위한 표준적인 방법**입니다. 전방전파, 손실 함수 계산, 역전파 과정을 거친 후, 경사 하강법을 통해 가중치와 편향을 업데이트합니다. 이 과정을 반복함으로써 신경망이 점점 더 좋은 성능을 갖게 됩니다.\n",
    "\n",
    "### 전체 흐름:\n",
    "1. **전방전파**: \n",
    "   - 입력 데이터를 이용해 예측값을 계산합니다.\n",
    "   - 예를 들어, 신경망을 통해 나온 출력값 \\( \\hat{Y} \\)를 얻습니다.\n",
    "\n",
    "2. **손실 함수 계산**:\n",
    "   - 예측값 \\( \\hat{Y} \\)와 실제값 \\( Y \\)를 비교하여 **오류(손실)**를 측정합니다.\n",
    "   - 손실 함수는 이 오류를 수치로 나타내며, 교차 엔트로피나 평균 제곱 오차(MSE) 등을 사용합니다.\n",
    "\n",
    "3. **역전파**:\n",
    "   - 손실 함수의 값을 이용해 **기울기(gradient)**를 계산합니다. 이는 각 가중치와 편향에 대한 손실 함수의 변화율을 나타냅니다.\n",
    "   - 이 과정에서 전방전파에서 계산된 값들이 다시 사용됩니다.\n",
    "\n",
    "4. **경사 하강법(Gradient Descent)**:\n",
    "   - **기울기 값**을 사용하여 가중치와 편향을 **업데이트**합니다. \n",
    "   - **학습률(learning rate)**을 곱한 기울기만큼 가중치와 편향을 조정하여, 신경망이 오류를 줄이도록 학습합니다.\n",
    "   - 예: \\( W = W - \\text{learning rate} \\times dW \\)\n",
    "\n",
    "5. **반복(iteration)**:\n",
    "   - 위의 과정을 여러 번 반복하여, 신경망이 점점 더 **오류를 줄이고** 더 정확한 예측을 할 수 있도록 합니다.\n",
    "\n",
    "따라서, **경사 하강법은 역전파 후 가중치와 편향을 업데이트하는 필수 과정**이며, 이를 통해 신경망이 학습하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_prev, W, b = cache\n",
    "m = A_prev.shape[1]\n",
    "\n",
    "# Calculate dW, db, and dA_prev\n",
    "dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "dA_prev = np.dot(W.T, dZ)\n",
    "# dW는 현재 레이어 가중치에 대한 기울기, 이는 출력 기울기 dZ와 이전 레이어의 출력 A_prev를 통해 계산된다\n",
    "# db는 현재 레이어 편향에 대한 기울기, dZ에 대해 행 방향으로 합산한 값\n",
    "# dA_prev는 이전 레이어의 출력값에 대한 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 및 편향 업데이트 공식: <br>\n",
    "W[l] = W[l] - learning_rate * dW[l] <br>\n",
    "b[l] = b[l] - learning_rate * db[l] <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문제에서는 주어진 주석에 맞춰 코드를 작성해야 하므로, 요구사항에 맞게 구현해 보겠습니다. 각 단계에서 필요한 코드를 정확히 채워 넣고, 주석을 기반으로 코드를 작성하겠습니다.\n",
    "\n",
    "### 수정된 코드\n",
    "\n",
    "1. **`parameters` 초기화**:\n",
    "   주어진 주석에 따라 가중치와 편향을 초기화합니다. 이 부분에서 사용하는 함수는 `initialize_parameters`로 가정합니다.\n",
    "\n",
    "```python\n",
    "parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "```\n",
    "\n",
    "2. **전방 전달**:\n",
    "   `A1, cache1`, `A2, cache2`에 대한 구현입니다. 첫 번째 레이어는 `ReLU` 활성화 함수, 두 번째 레이어는 `Sigmoid` 활성화 함수를 사용해야 합니다.\n",
    "\n",
    "```python\n",
    "A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"relu\")\n",
    "A2, cache2 = linear_activation_forward(A1, W2, b2, activation=\"sigmoid\")\n",
    "```\n",
    "\n",
    "3. **비용 함수 계산**:\n",
    "   비용 함수는 로지스틱 회귀의 비용 함수와 동일하게 계산됩니다.\n",
    "\n",
    "```python\n",
    "cost = compute_cost(A2, Y)\n",
    "```\n",
    "\n",
    "4. **역전파**:\n",
    "   역전파 과정에서는 `linear_activation_backward`를 사용하여 각 레이어에 대한 기울기를 계산합니다.\n",
    "\n",
    "```python\n",
    "dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation=\"sigmoid\")\n",
    "dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation=\"relu\")\n",
    "```\n",
    "\n",
    "5. **파라미터 업데이트**:\n",
    "   기울기를 사용해 파라미터를 업데이트하는 코드입니다.\n",
    "\n",
    "```python\n",
    "parameters = update_parameters(parameters, grads, learning_rate)\n",
    "```\n",
    "\n",
    "### 최종 수정된 코드\n",
    "\n",
    "```python\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []  \n",
    "    m = X.shape[1]  \n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation=\"sigmoid\")\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation=\"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation=\"relu\")\n",
    "        \n",
    "        # Set grads\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 iterations\n",
    "        if print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(f\"Cost after iteration {i}: {np.squeeze(cost)}\")\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs\n",
    "```\n",
    "\n",
    "이제 실행 코드에서 `initialize_parameters`, `linear_activation_forward`, `compute_cost`, `linear_activation_backward`, `update_parameters` 등의 함수를 정의하거나 이미 정의되어 있는 경우, 정상적으로 작동할 것입니다.\n",
    "\n",
    "추가로 제공된 실행 코드가 제대로 동작하는지 다시 한 번 확인해 보시길 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
