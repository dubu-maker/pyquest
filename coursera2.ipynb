{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 손실 함수\n",
    "Loss=−(y⋅log(y^)+(1−y)⋅log(1−y^ ))\n",
    "\n",
    "L(0.5,0)=−(0⋅log(0.5)+(1−0)⋅log(1−0.5))=−(log(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "물류 손실 값: 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 주어진 값\n",
    "y_hat = 0.5\n",
    "y = 0\n",
    "\n",
    "# 로지스틱 손실 함수 구현\n",
    "loss = -(y * np.log(y_hat) + (1 - y) * np.log(1 - y_hat))\n",
    "\n",
    "print(f\"물류 손실 값: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931)\n"
     ]
    }
   ],
   "source": [
    "# 파이토치\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 로지스틱 손실 함수 사용 예시 (PyTorch)\n",
    "loss_fn = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "y_hat = torch.tensor([0.5])\n",
    "y = torch.tensor([0.0])\n",
    "\n",
    "loss = loss_fn(y_hat, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array([[[1],[2]],[[3],[4]]])\n",
    "x.shape\n",
    "# 2개층, 2개의 행, 1개의 열"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 행렬 곱셈\n",
    "행렬 곱셈은 두 행렬을 결합하여 새로운 정보를 생성할 때 주로 사용된다. 선형변환, 데이터 변환 또는 신경망 계산에 사용된다\n",
    "# 요소별 곱셈\n",
    "두 배열의 같은 위치에 있는 값을 직접 곱하는 연산, 두 배열이 동일하거나 브로드캐스팅을 통해 같은 크기로 확장할 때 사용된다. 두 데이터를 독립적으로 결합할 때 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.dot(a, a)`는 **행렬 곱셈**을 수행하는 함수이며, 요소별 곱셈이 아닌 행렬의 내적(dot product)을 계산합니다. 행렬 곱셈에서는 각 요소에 대한 연산이 아니라 **행과 열을 기준으로 벡터의 내적**을 계산하게 됩니다. \n",
    "\n",
    "### 설명:\n",
    "1. \\( a = \\begin{pmatrix} 2 & 1 \\\\ 1 & 3 \\end{pmatrix} \\)\n",
    "   \n",
    "   이 행렬을 다시 한 번 자기 자신과 곱하는 것은 다음과 같이 계산됩니다:\n",
    "\n",
    "2. **행렬 곱셈의 규칙**:\n",
    "   - 결과 행렬의 각 요소는 **첫 번째 행렬의 행**과 **두 번째 행렬의 열** 간의 벡터 내적을 계산합니다.\n",
    "   \n",
    "   **결과**:\n",
    "   \\[\n",
    "   \\text{np.dot(a, a)} = \\begin{pmatrix} \n",
    "   (2 \\times 2 + 1 \\times 1) & (2 \\times 1 + 1 \\times 3) \\\\\n",
    "   (1 \\times 2 + 3 \\times 1) & (1 \\times 1 + 3 \\times 3)\n",
    "   \\end{pmatrix}\n",
    "   = \\begin{pmatrix} 5 & 5 \\\\ 5 & 10 \\end{pmatrix}\n",
    "   \\]\n",
    "\n",
    "### 계산 과정:\n",
    "- 첫 번째 행 첫 번째 열: \\( 2 \\times 2 + 1 \\times 1 = 4 + 1 = 5 \\)\n",
    "- 첫 번째 행 두 번째 열: \\( 2 \\times 1 + 1 \\times 3 = 2 + 3 = 5 \\)\n",
    "- 두 번째 행 첫 번째 열: \\( 1 \\times 2 + 3 \\times 1 = 2 + 3 = 5 \\)\n",
    "- 두 번째 행 두 번째 열: \\( 1 \\times 1 + 3 \\times 3 = 1 + 9 = 10 \\)\n",
    "\n",
    "따라서 최종적으로 결과 행렬은:\n",
    "\\[\n",
    "\\begin{pmatrix} 5 & 5 \\\\ 5 & 10 \\end{pmatrix}\n",
    "\\]\n",
    "\n",
    "이 연산은 **요소별 곱셈**이 아니라 **행렬 곱셈**에 해당하며, 각 행과 열의 벡터 내적을 계산하여 결과를 구하게 됩니다.\n",
    "\n",
    "### 내적과 행렬 곱셈의 차이:\n",
    "- **내적**: 두 벡터 간의 스칼라 값을 반환하는 연산입니다. \\( a \\cdot b \\)처럼 두 벡터의 각 성분을 곱하고 그 결과를 모두 더합니다.\n",
    "- **행렬 곱셈**: 행렬의 각 행과 열을 기준으로 여러 벡터 내적을 수행하여 결과 행렬을 얻습니다.\n",
    "\n",
    "따라서 행렬 곱셈은 내적을 여러 번 수행하는 방식으로 동작하며, `np.dot`은 이를 처리합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [1 3]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 5,  5],\n",
       "       [ 5, 10]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[2, 1], [1,3]])\n",
    "print(a)\n",
    "np.dot(a, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def basic_sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute sigmoid of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- A scalar\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + math.exp(-x))  # 시그모이드 수식 적용\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이로 시그모이드 함수 계산\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x.\n",
    "\n",
    "    Arguments:\n",
    "    x -- A scalar or numpy array of any size.\n",
    "\n",
    "    Return:\n",
    "    s -- sigmoid(x)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치로 시그모이드 함수 계산\n",
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + torch.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로 시그모이드 함수 계산\n",
    "import tensorflow as tf\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + tf.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# σ′(x)=σ(x)(1−σ(x))\n",
    "a(x)는 시그모이드 함수의 결과이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = 1 / (1 + np.exp(-x))는 시그모이드 함수의 결과이다.\n",
    "\n",
    "ds = s * (1 - s)는 시그모이드 함수의 기울기를 나타낸다. 즉 시그모이드 함수의 미분값\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 - 배열 변형(Reshaping arrays)\n",
    "딥러닝에서 자주 사용하는 두 가지 넘파이 함수는 np.shape와 np.reshape()입니다.\n",
    "\n",
    "X.shape는 행렬이나 벡터 \n",
    "𝑋\n",
    "X의 형태(차원)를 얻는 데 사용됩니다.\n",
    "**X.reshape(...)**는 \n",
    "𝑋\n",
    "X의 형태를 다른 차원으로 변형하는 데 사용됩니다.\n",
    "예를 들어, 컴퓨터 과학에서 이미지(image)는 \n",
    "(길이,높이,깊이=3)의 3차원 배열로 나타냅니다. 그러나 알고리즘의 입력으로 이미지를 읽을 때, 이미지의 형태를 \n",
    "(길이×높이×3,1)의 벡터로 변환해야 합니다. 다시 말해, 3차원 배열을 1차원 벡터로 \"펼치기(unroll)\" 또는 변형(reshape)하는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .reshape(-1, 1)는 자동으로 넘파이가 배열을 만든다.\n",
    "# (length*height*depth, 1)의 형태"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"np.reshape(a, newshape, order='C')\"\n",
    "# a: 변형할 배열\n",
    "# newshape: 배열의 새로운 형태를 정의하는 정수 또는 튜플\n",
    "# order: 배열을 변형하는 순서를 지정(C는 행우선, F는 열우선)\n",
    "\n",
    "'배열의 차원을 변경하지만, 데이터는 그대로 유지합니다.'\n",
    "arr = np.array([1, 2, 3, 4])\n",
    "reshaped = np.reshape(arr, (2, 2))  # 결과: [[1, 2], [3, 4]]\n",
    "\n",
    "# 자동 차원 계산:\n",
    "'-1을 사용하여 자동으로 차원을 계산할 수 있습니다.'\n",
    "arr = np.array([1, 2, 3, 4, 5, 6])\n",
    "reshaped = np.reshape(arr, (-1, 2))  # 결과: [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "'배열의 총 데이터 수는 변형 전후에 일치해야 합니다.'\n",
    "# 예를 들어, 원래 배열이 6개의 요소를 가지고 있다면, 새로운 형태도 6개의 요소를 가져야 합니다.\n",
    "\n",
    "# 다차원 배열에서도 사용할 수 있으며, 원하는 형태로 쉽게 변형할 수 있습니다.\n",
    "arr = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "reshaped = np.reshape(arr, (3, 2))  # 결과: [[1, 2], [3, 4], [5, 6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. L2 노름 (유클리드 노름)\n",
    "- **정의**: \n",
    "  \\[\n",
    "  \\|x\\|_2 = \\sqrt{x_1^2 + x_2^2 + \\ldots + x_n^2}\n",
    "  \\]\n",
    "벡터의 각 성분의 제곱의 합의 제곱근을 계산합니다. 이는 직관적으로 2차원 공간에서의 거리와 같으며, 일반적으로 가장 많이 사용됩니다.\n",
    "부드러운 경향을 가지며, 작은 변화에 대해 더 민감합니다\n",
    "- **사용 예**:\n",
    "  - **NumPy**:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "    x = np.array([1, 2, 3])\n",
    "    l2_norm = np.linalg.norm(x, ord=2)  # 또는 np.linalg.norm(x)\n",
    "    print(l2_norm)  # 결과: 3.7416573867739413\n",
    "    ```\n",
    "  - **PyTorch**:\n",
    "    ```python\n",
    "    import torch\n",
    "    x = torch.tensor([1.0, 2.0, 3.0])\n",
    "    l2_norm = torch.norm(x, p=2)  # 또는 torch.norm(x)\n",
    "    print(l2_norm)  # 결과: tensor(3.7417)\n",
    "    ```\n",
    "  - **TensorFlow**:\n",
    "    ```python\n",
    "    import tensorflow as tf\n",
    "    x = tf.constant([1.0, 2.0, 3.0])\n",
    "    l2_norm = tf.norm(x, ord='euclidean')  # 또는 tf.norm(x)\n",
    "    print(l2_norm.numpy())  # 결과: 3.7416573867739413\n",
    "    ```\n",
    "\n",
    "### 2. L1 노름 (맨해튼 노름)\n",
    "- **정의**: \n",
    "  \\[\n",
    "  \\|x\\|_1 = |x_1| + |x_2| + \\ldots + |x_n|\n",
    "  \\]\n",
    "각 성분의 절댓값의 합을 계산합니다. 이는 맨해튼 거리와 관련이 있습니다.\n",
    "큰 값에 의해 영향을 덜 받으며, 특히 희소성(sparsity) 있는 데이터를 다루는 데 효과적입니다.\n",
    "- **사용 예**:\n",
    "  - **NumPy**:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "    x = np.array([1, -2, 3])\n",
    "    l1_norm = np.linalg.norm(x, ord=1)\n",
    "    print(l1_norm)  # 결과: 6.0\n",
    "    ```\n",
    "  - **PyTorch**:\n",
    "    ```python\n",
    "    import torch\n",
    "    x = torch.tensor([1.0, -2.0, 3.0])\n",
    "    l1_norm = torch.norm(x, p=1)\n",
    "    print(l1_norm)  # 결과: tensor(6.)\n",
    "    ```\n",
    "  - **TensorFlow**:\n",
    "    ```python\n",
    "    import tensorflow as tf\n",
    "    x = tf.constant([1.0, -2.0, 3.0])\n",
    "    l1_norm = tf.norm(x, ord=1)\n",
    "    print(l1_norm.numpy())  # 결과: 6.0\n",
    "    ```\n",
    "\n",
    "### 3. L∞ 노름 (무한 노름)\n",
    "- **정의**: \n",
    "  \\[\n",
    "  \\|x\\|_\\infty = \\max(|x_1|, |x_2|, \\ldots, |x_n|)\n",
    "  \\]\n",
    "벡터의 성분 중에서 절댓값이 가장 큰 값을 선택합니다.\n",
    "- **사용 예**:\n",
    "  - **NumPy**:\n",
    "    ```python\n",
    "    import numpy as np\n",
    "    x = np.array([1, -2, 3])\n",
    "    l_inf_norm = np.linalg.norm(x, ord=np.inf)\n",
    "    print(l_inf_norm)  # 결과: 3.0\n",
    "    ```\n",
    "  - **PyTorch**:\n",
    "    ```python\n",
    "    import torch\n",
    "    x = torch.tensor([1.0, -2.0, 3.0])\n",
    "    l_inf_norm = torch.norm(x, p=float('inf'))\n",
    "    print(l_inf_norm)  # 결과: tensor(3.)\n",
    "    ```\n",
    "  - **TensorFlow**:\n",
    "    ```python\n",
    "    import tensorflow as tf\n",
    "    x = tf.constant([1.0, -2.0, 3.0])\n",
    "    l_inf_norm = tf.norm(x, ord=float('inf'))\n",
    "    print(l_inf_norm.numpy())  # 결과: 3.0\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정규화(Normalization)는 데이터의 스케일을 조정하여, 서로 다른 범위를 가진 데이터를 비교 가능하게 만들거나 머신러닝 모델의 성능을 개선하는 기법입니다. 정규화를 통해 데이터의 분포를 일관되게 하고, 학습 과정에서 특정 특징이 과도하게 영향을 미치지 않도록 할 수 있습니다. 일반적으로 사용되는 정규화 기법은 다음과 같습니다:\n",
    "\n",
    "### 1. 최소-최대 정규화 (Min-Max Normalization)\n",
    "- **정의**: 데이터의 최소값과 최대값을 이용해 모든 데이터를 [0, 1] 범위로 조정합니다.\n",
    "- **공식**: \n",
    "  \\[\n",
    "  x_{\\text{norm}} = \\frac{x - \\text{min}(x)}{\\text{max}(x) - \\text{min}(x)}\n",
    "  \\]\n",
    "\n",
    "### 2. Z-점수 정규화 (Z-score Normalization)\n",
    "- **정의**: 데이터의 평균을 0, 표준편차를 1로 조정하여 정규 분포를 따르도록 합니다.\n",
    "- **공식**: \n",
    "  \\[\n",
    "  x_{\\text{norm}} = \\frac{x - \\mu}{\\sigma}\n",
    "  \\]\n",
    "  여기서 \\( \\mu \\)는 평균, \\( \\sigma \\)는 표준편차입니다.\n",
    "\n",
    "### 3. L1 노름 정규화\n",
    "- **정의**: 각 특성의 절댓값의 합이 1이 되도록 조정합니다.\n",
    "- **공식**: \n",
    "  \\[\n",
    "  x_{\\text{norm}} = \\frac{x}{\\|x\\|_1}\n",
    "  \\]\n",
    "\n",
    "### 4. L2 노름 정규화\n",
    "- **정의**: 각 특성의 제곱합의 제곱근이 1이 되도록 조정합니다.\n",
    "- **공식**: \n",
    "  \\[\n",
    "  x_{\\text{norm}} = \\frac{x}{\\|x\\|_2}\n",
    "  \\]\n",
    "\n",
    "### 정규화의 이점\n",
    "- **학습 속도 향상**: 데이터의 스케일을 맞춤으로써 경량 하강법(gradient descent) 등의 최적화 알고리즘이 더 빠르게 수렴할 수 있습니다.\n",
    "- **특성의 균형**: 특정 특성이 과도하게 영향을 미치지 않도록 하여 모델의 일반화 성능을 높입니다.\n",
    "- **거리 기반 알고리즘의 성능 개선**: K-최근접 이웃(KNN)이나 군집화 알고리즘에서 거리 계산에 영향을 미치지 않도록 합니다.\n",
    "\n",
    "정규화는 특히 데이터의 분포가 다양할 때 중요하며, 머신러닝 모델의 성능을 향상시키는 데 도움을 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 행의 2-노름을 계산\n",
    "x_norm = np.linalg.norm(x, ord=2, axis=1, keepdims=True)\n",
    "# ord=2는 l2노름, ord=1은 l1노름\n",
    "# axis=1는 노름을 계산할 축을 지정한다. 즉 각 행 별로 노름을 계산한다. axis=0은 열에 대해 노름을 계산\n",
    "# keepdims=True: 결과의 차원수를 유지하도록 지시한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_exp = np.exp(x)\n",
    "x_sum = np.sum(x_exp, axis=1, keepdims=True)\n",
    "s = x_exp / x_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스 함수에서 지수 함수를 적용하고 나서 더하고 나누는 이유는 각 클래스에 대한 확률을 계산하기 위해서입니다. 이 과정을 좀 더 자세히 설명하자면:\n",
    "\n",
    "### 1. 지수 함수 적용 (`x_exp = np.exp(x)`)\n",
    "- **의미**: 입력 점수 `x`의 각 원소에 지수 함수를 적용하여 \\( e^{x_i} \\)를 계산합니다.\n",
    "- **이유**: 지수 함수는 원래의 점수를 양수로 변환하며, 큰 점수에 대해 더 큰 값을 생성합니다. 이로 인해 각 클래스의 상대적인 \"확신\" 정도를 강조합니다.\n",
    "\n",
    "### 2. 합계 계산 (`x_sum = np.sum(x_exp, axis=1, keepdims=True)`)\n",
    "- **의미**: 각 행의 지수 함수 값의 합을 계산합니다.\n",
    "- **이유**: 소프트맥스는 각 클래스의 지수 값이 전체 클래스 지수 값에서 차지하는 비율을 확률로 표현하기 위해 필요합니다. 이 합계는 나중에 각 클래스의 확률을 계산하는 데 사용됩니다.\n",
    "\n",
    "### 3. 나누기 (`s = x_exp / x_sum`)\n",
    "- **의미**: 각 클래스의 지수 값 \\( e^{x_i} \\)를 해당 행의 합계 \\( \\sum_{j} e^{x_j} \\)로 나눕니다.\n",
    "- **이유**: 이 나누기는 각 클래스의 지수 값이 전체 클래스 지수 값에서 차지하는 비율을 계산하여, 최종적으로 확률을 구하는 과정입니다. 이 결과는 항상 0과 1 사이의 값을 가지며, 각 행의 모든 확률의 합은 1이 됩니다.\n",
    "\n",
    "### 확률 계산 예시\n",
    "- 예를 들어, \\( x = [2.0, 1.0, 0.1] \\)라고 할 때:\n",
    "  1. \\( x_{\\text{exp}} = [e^{2.0}, e^{1.0}, e^{0.1}] \\)를 계산합니다.\n",
    "  2. 이들의 합을 구합니다: \\( x_{\\text{sum}} = e^{2.0} + e^{1.0} + e^{0.1} \\).\n",
    "  3. 각 원소를 합으로 나누어 확률을 얻습니다:\n",
    "     \\[\n",
    "     s = \\left[\\frac{e^{2.0}}{x_{\\text{sum}}}, \\frac{e^{1.0}}{x_{\\text{sum}}}, \\frac{e^{0.1}}{x_{\\text{sum}}}\\right]\n",
    "     \\]\n",
    "\n",
    "이렇게 함으로써 각 클래스의 상대적인 확률을 표현하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1 손실 함수**와 **L1 노름**은 유사하지만, 개념적으로 약간 다릅니다. 둘 다 절댓값 합을 사용하는 공통점이 있지만, 사용 목적과 맥락이 다릅니다.\n",
    "\n",
    "### 1. L1 손실 함수 (L1 Loss Function)\n",
    "- **목적**: 모델의 예측값과 실제 값 사이의 차이를 측정하는 데 사용됩니다.\n",
    "- **정의**: 주로 머신러닝에서 예측값( \\( \\hat{y} \\) )과 실제 값( \\( y \\) ) 간의 절대 차이의 합을 계산하는 함수입니다. 모델이 얼마나 잘 예측했는지 평가하는 기준으로 사용됩니다.\n",
    "  \n",
    "  \\[\n",
    "  L_1(\\hat{y}, y) = \\sum_{i=0}^{m-1} |y^{(i)} - \\hat{y}^{(i)}|\n",
    "  \\]\n",
    "\n",
    "  **사용 사례**: L1 손실 함수는 회귀 문제에서 자주 사용되며, 예측 값과 실제 값의 차이를 절댓값으로 측정합니다. 딥러닝이나 머신러닝에서 모델의 성능을 평가하고, 그 성능을 개선하기 위해 손실을 최소화하는 방식으로 사용됩니다.\n",
    "\n",
    "### 2. L1 노름 (L1 Norm)\n",
    "- **목적**: 벡터의 크기를 측정하는 방법 중 하나입니다. 주로 벡터의 **크기(길이)**를 계산하는 데 사용됩니다.\n",
    "- **정의**: 벡터의 각 성분의 절댓값을 모두 더한 값입니다. 벡터가 원점으로부터 얼마나 떨어져 있는지를 측정하는 방식입니다.\n",
    "\n",
    "  \\[\n",
    "  \\| x \\|_1 = \\sum_{i=0}^{n-1} |x_i|\n",
    "  \\]\n",
    "\n",
    "  **사용 사례**: L1 노름은 선형대수학에서 벡터의 크기를 측정하거나, 머신러닝에서 **정규화(regularization)** 기법으로 사용됩니다. 특히 Lasso 회귀에서 L1 노름을 통해 모델의 가중치(특성)를 희소하게 만들 수 있습니다. 즉, 모델이 중요하지 않은 변수들의 가중치를 0으로 만들어 가독성을 높이고, 과적합을 방지합니다.\n",
    "\n",
    "### 차이점:\n",
    "- **L1 손실 함수**는 모델의 **예측 오류**를 측정하는 데 사용되며, **L1 노름**은 벡터의 **크기(길이)**를 계산하는 데 사용됩니다.\n",
    "- **L1 손실 함수**는 머신러닝에서 **예측 성능 평가**를 위해 사용되며, **L1 노름**은 벡터의 성질을 설명하거나 **정규화**에 사용됩니다.\n",
    "\n",
    "따라서, 두 개념은 절댓값 합을 사용한다는 점에서는 유사하지만, **L1 손실 함수**는 모델 평가에, **L1 노름**은 벡터의 크기 또는 특성 정규화에 주로 사용된다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 넘파이를 사용한 손실함수 계산\n",
    "import numpy as np\n",
    "\n",
    "# 예측값 (y_hat)과 실제값 (y)\n",
    "y_hat = np.array([3.0, -0.5, 2.0, 7.0])\n",
    "y = np.array([2.5, 0.0, 2.0, 8.0])\n",
    "\n",
    "# L1 손실 함수\n",
    "l1_loss = np.sum(np.abs(y - y_hat))\n",
    "\n",
    "print(\"L1 Loss:\", l1_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 손실 함수 (평균 제곱 오차)\n",
    "l2_loss = np.mean((y - y_hat)**2)\n",
    "\n",
    "print(\"L2 Loss:\", l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 손실 함수\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 예측값 (y_hat)과 실제값 (y)\n",
    "y_hat = torch.tensor([3.0, -0.5, 2.0, 7.0])\n",
    "y = torch.tensor([2.5, 0.0, 2.0, 8.0])\n",
    "\n",
    "# L1 손실 함수 정의\n",
    "l1_loss_fn = nn.L1Loss()\n",
    "\n",
    "# L1 손실 계산\n",
    "l1_loss = l1_loss_fn(y_hat, y)\n",
    "print(\"L1 Loss:\", l1_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 손실 함수 정의\n",
    "l2_loss_fn = nn.MSELoss()\n",
    "\n",
    "# L2 손실 계산\n",
    "l2_loss = l2_loss_fn(y_hat, y)\n",
    "print(\"L2 Loss:\", l2_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1054)\n"
     ]
    }
   ],
   "source": [
    "# 파이토치\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 로지스틱 손실 함수 사용 예시 (PyTorch)\n",
    "loss_fn = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "y_hat = torch.tensor([0.9])\n",
    "y = torch.tensor([1.0])\n",
    "\n",
    "loss = loss_fn(y_hat, y)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(≈ 3 lines of code)\n",
    "m_train = train_set_x_orig.shape[0]\n",
    "m_test = test_set_x_orig.shape[0]\n",
    "num_px = train_set_x_orig.shape[1]\n",
    "\n",
    "# YOUR CODE ENDS HERE\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론이죠! 이 코드는 이미지 데이터를 로드하고, 훈련 및 테스트 데이터셋의 정보를 출력하는 과정을 보여줍니다. 각 부분을 설명해드릴게요.\n",
    "\n",
    "1. **필요한 라이브러리 임포트:**\n",
    "   ```python\n",
    "   import numpy as np\n",
    "   import copy\n",
    "   import matplotlib.pyplot as plt\n",
    "   import h5py\n",
    "   import scipy\n",
    "   from PIL import Image\n",
    "   from scipy import ndimage\n",
    "   from lr_utils import load_dataset\n",
    "   from public_tests import *\n",
    "   ```\n",
    "   - `numpy`: 배열 및 행렬 연산을 위한 라이브러리.\n",
    "   - `matplotlib.pyplot`: 데이터 시각화를 위한 라이브러리.\n",
    "   - `h5py`: HDF5 파일 형식을 읽고 쓰기 위한 라이브러리.\n",
    "   - `PIL`: 이미지를 처리하기 위한 라이브러리.\n",
    "   - `scipy`: 과학적 계산을 위한 라이브러리.\n",
    "   - `lr_utils`와 `public_tests`: 사용자 정의 함수가 포함된 모듈로, 주로 데이터 로딩 및 테스트를 위한 함수들.\n",
    "\n",
    "2. **Jupyter Notebook 설정:**\n",
    "   ```python\n",
    "   %matplotlib inline\n",
    "   %load_ext autoreload\n",
    "   %autoreload 2\n",
    "   ```\n",
    "   - `%matplotlib inline`: Jupyter Notebook에서 그래프를 인라인으로 표시.\n",
    "   - `%load_ext autoreload`와 `%autoreload 2`: 코드를 수정할 때마다 자동으로 모듈을 다시 로드.\n",
    "\n",
    "3. **데이터 로드:**\n",
    "   ```python\n",
    "   train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()\n",
    "   ```\n",
    "   - `load_dataset()` 함수는 훈련 및 테스트 데이터셋, 클래스 레이블을 로드합니다. 여기서 `train_set_x_orig`는 훈련 이미지, `train_set_y`는 훈련 레이블, `test_set_x_orig`와 `test_set_y`는 테스트 이미지와 레이블입니다.\n",
    "\n",
    "4. **이미지 시각화:**\n",
    "   ```python\n",
    "   index = 25\n",
    "   plt.imshow(train_set_x_orig[index])\n",
    "   print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")\n",
    "   ```\n",
    "   - `index = 25`: 25번째 이미지를 선택.\n",
    "   - `plt.imshow(...)`: 해당 이미지를 시각화.\n",
    "   - `print(...)`: 해당 이미지의 레이블을 출력.\n",
    "\n",
    "5. **데이터의 크기 확인:**\n",
    "   ```python\n",
    "   m_train = train_set_x_orig.shape[0]\n",
    "   m_test = test_set_x_orig.shape[0]\n",
    "   num_px = train_set_x_orig.shape[1]\n",
    "   ```\n",
    "   - `m_train`: 훈련 데이터의 샘플 수.\n",
    "   - `m_test`: 테스트 데이터의 샘플 수.\n",
    "   - `num_px`: 각 이미지의 픽셀 수 (정사각형 이미지이므로 높이와 너비가 동일).\n",
    "\n",
    "6. **결과 출력:**\n",
    "   ```python\n",
    "   print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "   print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "   print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "   print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "   print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
    "   print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "   print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
    "   print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "   ```\n",
    "   - 훈련 및 테스트 데이터의 크기와 이미지의 크기 정보를 출력합니다. 이미지의 크기는 `(num_px, num_px, 3)`으로, 3은 RGB 채널을 의미합니다.\n",
    "\n",
    "이 코드를 통해 훈련 및 테스트 데이터의 구조를 이해하고, 이미지 데이터를 처리하기 위한 준비를 할 수 있습니다. 혹시 더 궁금한 부분이 있으면 말씀해 주세요!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 평탄화는 이미지 데이터를 머신러닝 모델에 입력하기 위한 중요한 과정입니다. 이미지가 다차원 배열로 표현되면 모델이 이를 처리하기 어렵기 때문에, 모든 픽셀 값을 하나의 벡터로 변환하여 입력 형식을 통일합니다.\n",
    "1. 원래 데이터 형태: 원본 이미지는 (num_px,num_px,3) 형태로, 각 이미지가 높이, 너비, 색상 채널(RGB)로 구성되어 있습니다.\n",
    "2. 행렬 평탄화: 각 이미지를 평탄화하여 (num_px×num_px×3,1) 형태로 변환합니다. 예를 들어, 64×64×3 크기의 이미지는 평탄화 후 (12288,1)로 변환됩니다.\n",
    "\n",
    "형태 변환: 평탄화 후 각 이미지의 픽셀 데이터는 원래의 다차원 배열에서 벡터로 변환됩니다. 이때, 전치를 통해 각 이미지를 열 벡터 형태로 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "# test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 예시로 사용할 임의의 이미지 데이터 생성 (3개의 이미지, 64x64 크기, RGB)\n",
    "num_images = 3\n",
    "num_px = 64\n",
    "images = np.random.randint(0, 256, (num_images, num_px, num_px, 3))  # (3, 64, 64, 3)\n",
    "\n",
    "# 평탄화\n",
    "images_flatten = images.reshape(images.shape[0], -1).T  # (64*64*3, 3)\n",
    "# .shape[0]: [0] 이미지의 샘플 수, -1: 나머지 차원수를 자동으로 계산하라\n",
    "\n",
    "# 결과 확인\n",
    "print(\"Original shape: \", images.shape)            # (3, 64, 64, 3)\n",
    "print(\"Flattened shape: \", images_flatten.shape)   # (12288, 3)\n",
    "\n",
    "# 첫 번째 이미지의 첫 10 픽셀 확인\n",
    "print(\"First 10 pixels of the first flattened image:\", images_flatten[0:10, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 데이터셋을 전처리할 때 일반적으로 필요한 단계는 다음과 같습니다:\n",
    "\n",
    "문제의 차원과 형태를 파악합니다 (m_train, m_test, num_px 등).\n",
    "각 예제가 이제 (num_px * num_px * 3, 1) 크기의 벡터가 되도록 데이터셋의 형태를 변경합니다.\n",
    "데이터를 \"표준화\"합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    이 함수는 w를 위해 (dim, 1) 형태의 0으로 이루어진 벡터를 생성하고, b를 0으로 초기화합니다.\n",
    "    \n",
    "    인수:\n",
    "    dim -- 우리가 원하는 w 벡터의 크기 (또는 이 경우 매개변수의 수)\n",
    "    \n",
    "    반환:\n",
    "    w -- (dim, 1) 형태로 초기화된 벡터\n",
    "    b -- 초기화된 스칼라 (바이어스에 해당)로 float 타입\n",
    "    \"\"\"\n",
    "    \n",
    "    # (≈ 2 줄의 코드)\n",
    "    w = np.zeros((dim, 1))  # w를 (dim, 1) 형태의 0으로 초기화\n",
    "    b = 0.0                 # b를 0.0으로 초기화\n",
    "    \n",
    "    return w, b\n",
    "dim은 매개 변수 벡터 w의 크기를 나타내는 값, 보통 dim에는 입력 특성의 수가 들어간다. 예를 들어 이미지의 경우 픽셀 수\n",
    "모든 값을 0으로 초기화 하는 이유: 매개 변수를 초기화 하여 모델이 시작할 때 편향이 없도록 한다, 수렴 안정성, 계산 간소화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    구현: 비용 함수와 기울기를 계산하는 함수\n",
    "\n",
    "    인수:\n",
    "    w -- 가중치, 크기 (num_px * num_px * 3, 1)의 넘파이 배열\n",
    "    b -- 편향, 스칼라\n",
    "    X -- 크기 (num_px * num_px * 3, 예제 수)의 데이터\n",
    "    Y -- 실제 \"레이블\" 벡터 (비고양이일 경우 0, 고양이일 경우 1) 크기 (1, 예제 수)\n",
    "\n",
    "    반환:\n",
    "    grads -- 가중치와 편향의 기울기를 포함하는 딕셔너리\n",
    "            (dw -- 손실에 대한 w의 기울기, 따라서 w와 동일한 형태)\n",
    "            (db -- 손실에 대한 b의 기울기, 따라서 b와 동일한 형태)\n",
    "    cost -- 로지스틱 회귀를 위한 음의 로그 우도 비용\n",
    "\n",
    "    팁:\n",
    "    - np.log(), np.dot()을 사용하여 단계별로 코드를 작성하세요.\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]  # 예제의 수를 얻습니다.\n",
    "    \n",
    "    # 순전파 (X에서 비용으로)\n",
    "    #(≈ 2줄의 코드)\n",
    "    # 활성화 계산\n",
    "    # A = ...\n",
    "    # np.dot을 사용하여 비용을 계산합니다. \n",
    "    # 합계를 위해 루프를 사용하지 마세요.\n",
    "    # cost = ...                                 \n",
    "    # YOUR CODE STARTS HERE\n",
    "    A = \n",
    "\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    # 역전파 (기울기를 찾기 위해)\n",
    "    #(≈ 2줄의 코드)\n",
    "    # dw = ...\n",
    "    # db = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    cost = np.squeeze(np.array(cost))  # 비용을 스칼라로 변환합니다.\n",
    "\n",
    "    grads = {\"dw\": dw,  # 가중치에 대한 기울기\n",
    "             \"db\": db}  # 편향에 대한 기울기\n",
    "    \n",
    "    return grads, cost  # 기울기와 비용을 반환합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w의 형태는 (num_px * num_px * 3, 1)입니다. 예를 들어, num_px가 64일 경우 w의 크기는 (12288, 1)입니다.\n",
    "w.T를 적용하면, w의 형태는 (1, num_px * num_px * 3)으로 바뀝니다. 즉, 한 줄로 늘어난 형태입니다.  w.T는 (1, 12288)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **데이터셋 로드**: 훈련과 테스트 데이터를 준비합니다.\n",
    "- **평탄화(Flattening)**: 다차원 데이터를 1차원 벡터로 변환합니다.\n",
    "- **파라미터 초기화**: 가중치와 편향을 초기화합니다.\n",
    "- **전방 전파(Forward Propagation)**: 시그모이드 함수를 사용하여 예측을 계산합니다.\n",
    "- **비용 함수 계산**: 로지스틱 손실 함수로 비용을 계산합니다.\n",
    "- **역방향 전파(Backward Propagation)**: 비용에 대한 경사도를 계산합니다.\n",
    "- **파라미터 업데이트**: 경사 하강법을 통해 가중치와 편향을 업데이트합니다.\n",
    "- **반복**: 최적화 과정을 반복하여 모델을 개선합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X -> A -> Z -> H -> Y\n",
    "- X: 입력 데이터 (예: 이미지나 텍스트 데이터 등)\n",
    "- A: 활성화 값(Activation)으로, 보통 신경망에서 각 층을 통과하면서 계산된 값입니다.\n",
    "- Z: 선형 결합의 결과, 즉 **Z = w^T X + b**로 계산된 값입니다. 이 값에 시그모이드 같은 활성화 함수가 적용됩니다.\n",
    "- H: 은닉층의 출력을 나타냅니다. 여러 층이 있는 경우 각 층에서의 출력이 H로 표현될 수 있습니다.\n",
    "- Y: 최종 출력 값 또는 예측 값입니다. 모델이 최종적으로 출력하는 결과입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 1000\n",
      "Image size (height/width): 64\n",
      "Shape of train_set_x_orig: (1000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 예를 들어, 1000개의 64x64 픽셀의 RGB 이미지로 이루어진 데이터셋\n",
    "train_set_x_orig = np.random.rand(1000, 64, 64, 3)  # 무작위로 생성된 1000개의 이미지\n",
    "test_set_x_orig = np.random.rand(200, 64, 64, 3)\n",
    "\n",
    "# 이미지 수, 가로 크기, 세로 크기, 채널 수 출력\n",
    "m_train = train_set_x_orig.shape[0]  # 1000\n",
    "num_px = train_set_x_orig.shape[1]   # 64\n",
    "\n",
    "print(\"Number of training examples:\", m_train)\n",
    "print(\"Image size (height/width):\", num_px)\n",
    "print(\"Shape of train_set_x_orig:\", train_set_x_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.35739603, 0.32026664, 0.73192828],\n",
       "       [0.15910299, 0.90814416, 0.89223818]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(2, 3) # 균등 분포에서 무작위 수를 생성한다. (0과 1 사이 무작위 값, 모든 값이 0과 1사이에서 동일한 확률로 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.29934094, -0.33047756, -1.63117818],\n",
       "       [-0.70313253,  0.78155265,  0.06274786]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(2, 3) # 정규 분포에서 무작위 수를 생성한다. (정규 분포는 종 모양, 값은 주로 평균값(0)에 가까운 곳에서 많이 생성되며 양쪽으로 멀어질 수록 확률이 낮아진다)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- rand: 항상 0과 1 사이의 값을 반환.\n",
    "- randn: 값이 음수, 양수 모두 나올 수 있으며, -3에서 +3 사이의 값이 대부분을 차지하지만 더 극단적인 값도 나올 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평탄화: 다차원 데이터를 1차원 벡터로 변환, 딥러닝에서는 입력데이터를 벡터로 처리해야 하기 때문\n",
    "\n",
    "# 신경망에서는 훈련 샘플이 열 단위로 들어가는 것이 일반적. 열이 하나의 샘플, 각 행이 해당 샘플의 특성을 나타낸다\n",
    "(1000, 4096) .T (4096, 1000)\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "# reshpae 함수: 배열의 모양을 바꿔주는 함수로 2, 3차원 배열을 쉽게 1차원 배열로 바꿔준다.\n",
    "'-1은 넘파이에서 자동으로 적절한 차원을 계산하라는 의미 (알아서 1차원으로 바꿔준다)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255.\n",
    "'0~1 사이의 값으로 바꾸는 과정, 데이터의 범위를 [0, 1]로 축소한다.'\n",
    "# 노름의 목적은 크기조절과 데이터의 일관성에 있다\n",
    "# L2노름 같은 정규화는 벡터의 크기를 1로 맞추는 것\n",
    "# **벡터의 크기(길이)**는 벡터의 **노름(norm)**으로 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파라미터 초기화 \n",
    "$$\n",
    "z = w^T X + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 초기화\n",
    "dim = 5\n",
    "w = np.zeros((dim, 1))\n",
    "b = 0.0\n",
    "# dim은 입력 데이터의 차원 수(특성 수), 모델이 입력으로 받는 데이터의 특성(또는 피처)의 개수를 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시그모이드 함수 (활성화 함수)\n",
    "$$\n",
    "s = \\frac{1}{1 + e^{-z}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Z는 선형 결합 Z = w^T X + b로 계산된 값, 이 값에 시그모이드 함수같은 활성화 함수가 적용된다.\n",
    "\n",
    "s = 1 / (1 + np.exp(-z)) \n",
    "'시그모이드 함수는 출력값을 항상 0과 1사이의 값으로 변환한다. 이진 분류에서 출력이 0.7이면 해당 클래스에 속할 확률이 70%라고 해석'\n",
    "# 출력이 0.5보다 크면 양성클래스(1), 0.5보다 작으면 음성클래스(0)으로 분류한다.\n",
    "'시그모이드 함수는 입력값에 대해 비선형적인 변환을 제공한다'\n",
    "\n",
    "'로지스틱 회귀의 활성화 함수: 시그모이드 함수'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치 시그모이드 함수\n",
    "import torch\n",
    "\n",
    "# 예시 데이터\n",
    "w = torch.randn(12288, 1)\n",
    "X = torch.randn(12288, 1000)\n",
    "b = torch.tensor(0.0)\n",
    "\n",
    "# 시그모이드 함수 적용\n",
    "Z = torch.mm(w.T, X) + b  # np.dot과 동일한 행렬 곱셈 함수\n",
    "A = torch.sigmoid(Z)      # 시그모이드 함수 적용\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로 시그모이드 함수\n",
    "import tensorflow as tf\n",
    "\n",
    "# 예시 데이터\n",
    "w = tf.random.normal([12288, 1])\n",
    "X = tf.random.normal([12288, 1000])\n",
    "b = tf.constant(0.0)\n",
    "\n",
    "# 시그모이드 함수 적용\n",
    "Z = tf.matmul(tf.transpose(w), X) + b  # np.dot과 동일한 행렬 곱셈 함수\n",
    "A = tf.sigmoid(Z)                      # 시그모이드 함수 적용\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로지스틱 회귀 함수 과정에서 전방 전파와 역방향 전파를 구현하는 함수\n",
    "'w는 가중치 벡터'\n",
    "'b는 편향(상수)'\n",
    "'X는 입력데이터(특성)'\n",
    "'Y는 실제 레이블(정답, 0 또는 1)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전방 전파\n",
    "'활성화 값 A 계산'\n",
    "X = np.random.rand(12288, 1000)  # 12288 (64*64*3) 크기의 특성을 가진 1000개의 샘플\n",
    "w = np.random.rand(12288, 1)     # 가중치 w는 X의 특성 수와 일치해야 함\n",
    "b = 0.0                          # 편향은 스칼라 값\n",
    "\n",
    "A = 1 / (1 + np.exp(-(np.dot(w.T, X) + b)))\n",
    "# A는 시그모이드 함수를 사용해 계산된 예측값, 0과 1사이의 확률 값을 구한다\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 로지스틱 회귀 함수의 비용 함수\n",
    "$$\n",
    "J(w,b)=− \n",
    "m\n",
    "1\n",
    "​\n",
    "  \n",
    "i=1\n",
    "∑\n",
    "m\n",
    "​\n",
    " [y \n",
    "(i)\n",
    " log(a \n",
    "(i)\n",
    " )+(1−y \n",
    "(i)\n",
    " )log(1−a \n",
    "(i)\n",
    " )]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 비용함수 계산\n",
    "cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
    "'로지스틱 회귀 함수의 비용함수(손실함수), 예측값 A와 실제값 Y사이의 차이를 나타내며, 이 값을 최소화 하려고 한다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18388253450393677\n"
     ]
    }
   ],
   "source": [
    "# 파이토치 내장 비용함수 계산\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 예시 데이터\n",
    "Y = torch.tensor([[1.0, 0.0, 1.0]])  # 실제 레이블 (크기: (1, 3))\n",
    "A = torch.tensor([[0.9, 0.2, 0.8]])  # 예측 확률 (크기: (1, 3))\n",
    "\n",
    "# Binary Cross Entropy Loss 사용\n",
    "criterion = nn.BCELoss()  # 교차 엔트로피 손실 함수\n",
    "loss = criterion(A, Y)    # 예측값과 실제값을 이용해 손실 계산\n",
    "\n",
    "print(loss.item())  # 손실 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18388253\n"
     ]
    }
   ],
   "source": [
    "# 텐서플로 내장 비용함수 계산\n",
    "import tensorflow as tf\n",
    "\n",
    "# 예시 데이터\n",
    "Y = tf.constant([[1.0, 0.0, 1.0]])  # 실제 레이블 (크기: (1, 3))\n",
    "A = tf.constant([[0.9, 0.2, 0.8]])  # 예측 확률 (크기: (1, 3))\n",
    "\n",
    "# Binary Cross Entropy Loss 사용\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "loss = bce(Y, A)  # 예측값과 실제값을 이용해 손실 계산\n",
    "\n",
    "print(loss.numpy())  # 손실 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역방향 전파 \n",
    "'가중치에 대한 경사도 dw 계산'\n",
    "dw = (1 / m) * np.dot(X, (A - Y).T) # dw는 벡터\n",
    "# A - Y 는 예측값과 실제 값 사이의 오차, 이를 기반으로 입력 데이터 X와의 곱을 통해 가중치 변화 방향을 구한다\n",
    "# 경사도는 로지스틱 회귀나 신경망에서 최적화를 위해 사용된다. 경사도는 비용함수가 어떻게 변화하는지를 보여준다.\n",
    "'경사도 계산의 목적: 가중치와 편향을 업데이트, 최적화 과정'\n",
    "\n",
    "# dw의 크기는 w와 같다. 즉 가중치 벡터의 크기에 따라 결정된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0004],\n",
      "        [-0.0001],\n",
      "        [ 0.0020],\n",
      "        ...,\n",
      "        [ 0.0008],\n",
      "        [ 0.0009],\n",
      "        [ 0.0003]])\n",
      "tensor([0.0050])\n"
     ]
    }
   ],
   "source": [
    "# 파이토치 경사도 계산\n",
    "import torch\n",
    "\n",
    "# 예시 데이터\n",
    "X = torch.randn(12288, 1000, requires_grad=True)  # 입력 데이터, requires_grad=True 설정\n",
    "Y = torch.randn(1, 1000)  # 실제 레이블\n",
    "w = torch.randn(12288, 1, requires_grad=True)  # 가중치, requires_grad=True 설정\n",
    "b = torch.randn(1, requires_grad=True)  # 편향\n",
    "\n",
    "# 전방 전파 계산\n",
    "Z = torch.mm(w.T, X) + b\n",
    "A = torch.sigmoid(Z)\n",
    "\n",
    "# 비용 함수 계산 (예시로 MSE 사용)\n",
    "loss = torch.mean((A - Y) ** 2)\n",
    "\n",
    "# 역방향 전파 (자동 경사도 계산)\n",
    "loss.backward()\n",
    "\n",
    "# 경사도 확인\n",
    "print(w.grad)  # w에 대한 경사도 dw\n",
    "print(b.grad)  # b에 대한 경사도 db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텐서플로 경사도 계산\n",
    "import tensorflow as tf\n",
    "\n",
    "# 예시 데이터\n",
    "X = tf.random.normal([12288, 1000])\n",
    "Y = tf.random.normal([1, 1000])\n",
    "w = tf.Variable(tf.random.normal([12288, 1]))  # 가중치\n",
    "b = tf.Variable(tf.random.normal([1]))  # 편향\n",
    "\n",
    "# 자동 미분을 위한 GradientTape\n",
    "with tf.GradientTape() as tape:\n",
    "    Z = tf.matmul(tf.transpose(w), X) + b\n",
    "    A = tf.sigmoid(Z)\n",
    "    loss = tf.reduce_mean(tf.square(A - Y))  # 예시로 MSE 사용\n",
    "\n",
    "# 경사도 계산\n",
    "dw, db = tape.gradient(loss, [w, b])\n",
    "\n",
    "# 경사도 출력\n",
    "print(dw)  # w에 대한 경사도\n",
    "print(db)  # b에 대한 경사도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 편향에 대한 경사도 db 계산\n",
    "db = (1 / m) * np.sum(A - Y)\n",
    "'편향 b에 대한 경사도 예측값과 실제값의 차이를 더한 후 평균을 구하여 편향의 업데이트 방향을 결정한다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향 초기화\n",
    "w, b = np.zeros((X_train.shape[0], 1)), 0\n",
    "'w 가중치 벡터를 0으로 초기화한다. .shape[0]은 특성의 개수'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "# optimize() 함수는 주어진 가중치 w와 편향 b를 사용하여 모델을 학습하고, 최적화한다\n",
    "# X_train, Y_train: 훈련 데이터와 레이블을 사용하여 가중치 w와 편향 b를 최적화한다.\n",
    "# num_iterations: 최적화를 반복할 횟수를 나타낸다.\n",
    "# learning_rate: 학습 속도를 결정하는 하이퍼파라미터\n",
    "# print_cost: 비용 함수를 출력할지 여부를 결정하는 매개변수\n",
    "\n",
    "# params: 최적화된 가중치 w와 편향 b를 포함하는 딕셔너리\n",
    "# grads: 가중치와 편향에 대한 경사도(gradient)\n",
    "# costs: 각 반복에서의 비용 함수 값을 저장한 리스트로, 학습 과정에서 비용이 어떻게 감소하는지를 확인할 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치와 편향 업데이트\n",
    "w = params[\"w\"]\n",
    "b = params[\"b\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 예측\n",
    "Y_prediction_test = predict(w, b, X_test)\n",
    "# w와 b는 이미 최적화된 값이기 때문에, 이를 바탕으로 **X_test**에 대해 예측을 진행\n",
    "# Y_prediction_test**는 테스트 데이터의 각 샘플에 대해 0 또는 1로 예측된 레이블을 나타낸다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 예측\n",
    "Y_prediction_train = predict(w, b, X_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
