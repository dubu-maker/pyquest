{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physical devices cannot be modified after being initialized\n",
      "X_train shape: (3072, 50000)\n",
      "X_test shape: (3072, 10000)\n",
      "Cost after iteration 0: 34657.35902819863\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# GPU ì‚¬ìš© ì„¤ì •\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # GPU ë©”ëª¨ë¦¬ ì¦ê°€ í—ˆìš©\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        tf.config.set_visible_devices(gpus, 'GPU')\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(f\"{len(gpus)} Physical GPUs, {len(logical_gpus)} Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "\n",
    "### 1. **ë°ì´í„°ì…‹ ë¡œë“œ**\n",
    "# ìš°ì„ , ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ë‹¤ë£¨ëŠ” ê²½ìš°, ì¼ë°˜ì ìœ¼ë¡œ í”½ì…€ ê°’ì„ íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  ê° ì´ë¯¸ì§€ë¥¼ í‰íƒ„í™”(flatten)í•´ì•¼ í•©ë‹ˆë‹¤. ê°€ë ¹, 64x64 RGB ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "\n",
    "#### ì˜ˆì‹œ (ì´ë¯¸ì§€ ë°ì´í„° ë¡œë“œ):\n",
    "# ì˜ˆì‹œë¡œ CIFAR-10 ë°ì´í„°ë¥¼ ë¡œë“œ\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ (ì´ë¯¸ì§€ í¬ê¸°: 32x32x3)\n",
    "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = cifar10.load_data()\n",
    "\n",
    "# 0 ë˜ëŠ” 1ì˜ ë ˆì´ë¸”ë¡œ ë³€í™˜ (ì˜ˆë¥¼ ë“¤ì–´ ê³ ì–‘ì´ì™€ ë‚˜ë¨¸ì§€ êµ¬ë¶„)\n",
    "Y_train = (Y_train_orig == 3).astype(int)  # ê³ ì–‘ì´ê°€ 3ë²ˆ í´ë˜ìŠ¤\n",
    "Y_test = (Y_test_orig == 3).astype(int)\n",
    "\n",
    "\n",
    "### 2. **ë°ì´í„° ì „ì²˜ë¦¬ ë° í‰íƒ„í™” (Flattening)**\n",
    "# ì´ë¯¸ì§€ë¥¼ ì‹ ê²½ë§ì— ì…ë ¥í•˜ê¸° ìœ„í•´ 2D ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë˜í•œ, í”½ì…€ ê°’ì„ 0ê³¼ 1 ì‚¬ì´ë¡œ ì •ê·œí™”í•˜ì—¬ í•™ìŠµì´ ë” ì˜ë˜ë„ë¡ í•©ë‹ˆë‹¤.\n",
    "\n",
    "# í›ˆë ¨ ë°ì´í„° í‰íƒ„í™”\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1).T\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1).T\n",
    "\n",
    "# ë°ì´í„° ì •ê·œí™” (0 ~ 1 ë²”ìœ„ë¡œ ë§Œë“¤ê¸° ìœ„í•´ 255ë¡œ ë‚˜ëˆ”)\n",
    "X_train = X_train_flatten / 255.\n",
    "X_test = X_test_flatten / 255.\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "### 3. **ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ì´ˆê¸°í™”**\n",
    "# ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ì—ì„œ ê°€ì¤‘ì¹˜ `w`ì™€ í¸í–¥ `b`ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "w = np.zeros((X_train.shape[0], 1))  # íŠ¹ì„± ìˆ˜ ë§Œí¼ ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "b = 0.0  # í¸í–¥ ì´ˆê¸°í™”\n",
    "\n",
    "### 4. **ì „ë°© ì „íŒŒ (Forward Propagation)**\n",
    "# `w`ì™€ `b`ë¥¼ ì‚¬ìš©í•´ ì „ë°© ì „íŒŒë¥¼ ê³„ì‚°í•˜ê³ , **ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜**ë¥¼ í†µí•´ ì˜ˆì¸¡ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # ì „ë°© ì „íŒŒ\n",
    "    A = sigmoid(np.dot(w.T, X) + b)  # í™œì„±í™” ê°’ A ê³„ì‚°\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))  # ë¹„ìš© í•¨ìˆ˜ ê³„ì‚°\n",
    "    \n",
    "    # ì—­ë°©í–¥ ì „íŒŒ (ê²½ì‚¬ë„ ê³„ì‚°)\n",
    "    dw = (1 / m) * np.dot(X, (A - Y).T)\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "    \n",
    "    cost = np.squeeze(cost)\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "### 5. **ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•œ ìµœì í™”**\n",
    "#ì „ë°© ì „íŒŒì™€ ì—­ë°©í–¥ ì „íŒŒë¥¼ í†µí•´ ê³„ì‚°í•œ ê²½ì‚¬ë„ë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # ë¹„ìš© ì €ì¥\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "    \n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n",
    "### 6. **ì˜ˆì¸¡ (Prediction)**\n",
    "# í•™ìŠµëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì‚¬ìš©í•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "def predict(w, b, X):\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
    "    \n",
    "    return Y_prediction\n",
    "\n",
    "### 7. **ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡**\n",
    "# ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "# í•™ìŠµ (ê²½ì‚¬ í•˜ê°•ë²• ìµœì í™”)\n",
    "num_iterations = 2000\n",
    "learning_rate = 0.005\n",
    "params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate)\n",
    "\n",
    "# ìµœì í™”ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥\n",
    "w = params[\"w\"]\n",
    "b = params[\"b\"]\n",
    "\n",
    "# ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "Y_prediction_test = predict(w, b, X_test)\n",
    "Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "# ì •í™•ë„ ê³„ì‚°\n",
    "train_accuracy = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n",
    "test_accuracy = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}%\")\n",
    "print(f\"Test accuracy: {test_accuracy}%\")\n",
    "\n",
    "### ì „ì²´ ê³¼ì • ìš”ì•½:\n",
    "# 1. **ë°ì´í„°ì…‹ ë¡œë“œ**: í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "# 2. **ë°ì´í„° ì „ì²˜ë¦¬**: ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  ì •ê·œí™”í•©ë‹ˆë‹¤.\n",
    "# 3. **ê°€ì¤‘ì¹˜ì™€ í¸í–¥ ì´ˆê¸°í™”**: ê°€ì¤‘ì¹˜ `w`ì™€ í¸í–¥ `b`ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "# 4. **ì „ë°© ì „íŒŒ(Forward Propagation)**: ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì˜ˆì¸¡ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "# 5. **ë¹„ìš© í•¨ìˆ˜ ê³„ì‚°**: ë¡œì§€ìŠ¤í‹± ì†ì‹¤ í•¨ìˆ˜ë¡œ ë¹„ìš©ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "# 6. **ê²½ì‚¬ í•˜ê°•ë²•(Backward Propagation)**: ê²½ì‚¬ë„ë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "# 7. **ì˜ˆì¸¡**: í•™ìŠµëœ ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì™€ í›ˆë ¨ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ê³ , ì •í™•ë„ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "# ì´ ê³¼ì •ì„ í†µí•´ **ë¡œì§€ìŠ¤í‹± íšŒê·€** ëª¨ë¸ì„ ì‚¬ìš©í•´ ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ê³ , ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "z=w \n",
    "T\n",
    " X+b\n",
    " $$\n",
    "ì„ í˜• í•¨ìˆ˜ zë¥¼ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ë¹„ì„ í˜• í•¨ìˆ˜ë¡œ ë§Œë“ ë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 1. ë°ì´í„° ë¡œë“œ (cifar10) ë° ì „ì²˜ë¦¬\n",
    "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = tf.keras.datasets.cifar10.load_data()\n",
    "\n",
    "# ê³ ì–‘ì´ë§Œ ë¶„ë¥˜í•˜ëŠ” ì´ì§„ ë¶„ë¥˜ ë¬¸ì œë¡œ ë³€í™˜ (3ë²ˆ í´ë˜ìŠ¤)\n",
    "Y_train = (Y_train_orig == 3).astype(int)\n",
    "Y_test = (Y_test_orig == 3).astype(int)\n",
    "\n",
    "# ì´ë¯¸ì§€ í‰íƒ„í™” ë° ì •ê·œí™”\n",
    "X_train_flatten = X_train_orig.reshape(X_train_orig.shape[0], -1) / 255.\n",
    "X_test_flatten = X_test_orig.reshape(X_test_orig.shape[0], -1) / 255.\n",
    "\n",
    "# 2. TensorFlow ëª¨ë¸ ìƒì„±\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train_flatten.shape[1],)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # ë¡œì§€ìŠ¤í‹± íšŒê·€\n",
    "])\n",
    "\n",
    "# 3. ëª¨ë¸ ì»´íŒŒì¼ (ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì„¤ì •)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),\n",
    "              loss='binary_crossentropy',  # ì´ì§„ ë¶„ë¥˜ë¥¼ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. ëª¨ë¸ í•™ìŠµ\n",
    "history = model.fit(X_train_flatten, Y_train, epochs=10, batch_size=64, validation_data=(X_test_flatten, Y_test))\n",
    "\n",
    "# 5. ê²°ê³¼ í™•ì¸\n",
    "train_loss, train_accuracy = model.evaluate(X_train_flatten, Y_train)\n",
    "test_loss, test_accuracy = model.evaluate(X_test_flatten, Y_test)\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy * 100:.2f}%\")\n",
    "print(f\"Test accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ (ì´ë¯¸ì§€ í¬ê¸°: 32x32x3)\n",
    "(X_train_orig, Y_train_orig), (X_test_orig, Y_test_orig) = cifar10.load_data()\n",
    "\n",
    "# CIFAR-10 ë°ì´í„°ì…‹ì˜ í´ë˜ìŠ¤ (0~9)\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# 5ê°œì˜ ì´ë¯¸ì§€ì™€ ë¼ë²¨ì„ í‘œì‹œ\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(5):\n",
    "    plt.subplot(1, 5, i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train_orig[i])\n",
    "    plt.xlabel(class_names[int(Y_train_orig[i])])  # í•´ë‹¹ ì´ë¯¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤ëª…\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " ZëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ì…ë ¥ê°’ (íŠ¹ì • ì¸µì—ì„œ ê°€ì¤‘ì¹˜ì™€ ë°”ì´ì–´ìŠ¤ë¥¼ ì ìš©í•œ ì„ í˜• ë³€í™˜ì˜ ê²°ê³¼)\n",
    " a^[1]ëŠ” zì— í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•œ ê²°ê³¼ê°’ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë§¤ê°œë³€ìˆ˜(ê°€ì¤‘ì¹˜)ë¥¼ ëª¨ë‘ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´ ì•ˆ ë˜ëŠ” ì´ìœ ëŠ” ì‹ ê²½ë§ í•™ìŠµì—ì„œ **ëŒ€ì¹­ì„± ê¹¨ê¸°(symmetry breaking)**ê°€ í•„ìš”í•˜ê¸° ë•Œë¬¸\n",
    "- ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë©´, ëª¨ë“  ë‰´ëŸ°ì´ ë™ì¼í•œ ê¸°ìš¸ê¸°ë¥¼ ê°€ì§€ê²Œ ë˜ë©°, ëª¨ë“  ë‰´ëŸ°ì´ ê°™ì€ í•™ìŠµì„ í•´ì„œ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í•œë‹¤\n",
    "- ê²½ì‚¬ í•˜ê°•ë²•ì˜ ë¬¸ì œ: ì—­ì „íŒŒì‹œ ê²½ì‚¬ë„ë¥¼ ê³„ì‚°í•  ë•Œ ë‰´ëŸ°ì— ëŒ€í•´ ë™ì¼í•œ ê°’ì´ ì „ë‹¬ëœë‹¤. ì´ ê²½ìš° ë„¤íŠ¸ì›Œí¬ê°€ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í•˜ë©°, ë‰´ë ¨ê°„ì˜ ì°¨ë³„ì„±ì´ ì—†ì–´ì§„ë‹¤\n",
    "\n",
    "ë”°ë¼ì„œ ë¬´ì‘ìœ„ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ë‰´ëŸ°ë“¤ì´ ì„œë¡œ ë‹¤ë¥¸ í•™ìŠµì„ í•  ìˆ˜ ìˆê²Œ í•´ì•¼ í•œë‹¤.\n",
    " **ì‘ì€ ë¬´ì‘ìœ„ ê°’ìœ¼ë¡œ ** ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì‹ ê²½ë§ì˜ í™œì„±í™” ê°’ê³¼ ê´€ë ¨ëœ í‘œê¸°ë²•\n",
    "- ğ‘[2](12)ëŠ” 12ë²ˆì§¸ í›ˆë ¨ ì˜ˆì œì—ì„œ 2ë²ˆì§¸ ì¸µì˜ í™œì„±í™” ë²¡í„°ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\n",
    "- XëŠ” ê° ì—´ì´ í•˜ë‚˜ì˜ í›ˆë ¨ ì˜ˆì œ\n",
    "- ğ‘4[2]ëŠ” 2ë²ˆì§¸ ì¸µì˜ 4ë²ˆì§¸ ë‰´ëŸ°ì— ì˜í•œ í™œì„±í™” ì¶œë ¥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$a_i^{[l]}$\n",
    " : ì—¬ê¸°ì„œ\n",
    "**\n",
    "ğ‘™\n",
    "**ì€ ì¸µ ë²ˆí˜¸ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¦‰, ëª‡ ë²ˆì§¸ ì¸µì˜ í™œì„±í™” ê°’ì¸ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. <br>\n",
    "**\n",
    "ğ‘–\n",
    "**ëŠ” í•´ë‹¹ ì¸µì—ì„œ ëª‡ ë²ˆì§¸ ë‰´ëŸ°ì— í•´ë‹¹í•˜ëŠ” í™œì„±í™” ê°’ì¸ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. <br>\n",
    "$a_i^{[l]}(m)$ì˜ í˜•íƒœëŠ” më²ˆì§¸ í›ˆë ¨ ì˜ˆì œì— ëŒ€í•œ í•´ë‹¹ ë‰´ë ¨ì˜ í™œì„±í™” ê°’ì„ ì˜ë¯¸í•œë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.random.randn(4, 3)\n",
    "print(A)\n",
    "# AëŠ” ë‹¤ìŒê³¼ ê°™ì€ 4x3 ë°°ì—´ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "# [[ 0.5,  1.2, -0.3],\n",
    "#  [-1.1,  0.4,  0.8],\n",
    "#  [ 0.9, -0.6,  1.5],\n",
    "#  [ 1.0,  1.1,  0.2]]\n",
    "\n",
    "B = np.sum(A, axis=1, keepdims=True)\n",
    "print(B)\n",
    "# BëŠ” ê° í–‰ì˜ í•©ê³„ë¥¼ ìœ ì§€í•œ (4, 1) ë°°ì—´ì…ë‹ˆë‹¤.\n",
    "# [[ 1.4],\n",
    "#  [ 0.1],\n",
    "#  [ 1.8],\n",
    "#  [ 2.3]]\n",
    "\n",
    "'keepdims=TrueëŠ” ì°¨ì›ì„ ìœ ì§€í•˜ë¼ëŠ” ì˜µì…˜, ê¸°ë³¸ì ìœ¼ë¡œ np.sum()ì€ ì°¨ì›ì„ ì œê±°í•œë‹¤.'\n",
    "B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ì…ë ¥ ë°ì´í„°: 4ê°œì˜ ì…ë ¥ ë‰´ëŸ° (x1, x2, x3, x4)\n",
    "X = np.random.randn(4, 1)  # (4, 1) í¬ê¸°ì˜ ì…ë ¥ ë°ì´í„°\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ: 2ê°œì˜ ë‰´ëŸ° (a1_1, a1_2)\n",
    "# W[1]: (2, 4) í¬ê¸°ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ (2ê°œì˜ ì¶œë ¥ ë‰´ëŸ°, 4ê°œì˜ ì…ë ¥ ë‰´ëŸ°)\n",
    "W1 = np.random.randn(2, 4)\n",
    "\n",
    "# b[1]: (2, 1) í¬ê¸°ì˜ í¸í–¥ ë²¡í„° (2ê°œì˜ ì¶œë ¥ ë‰´ëŸ°ì— ëŒ€ì‘í•˜ëŠ” í¸í–¥)\n",
    "b1 = np.random.randn(2, 1)\n",
    "\n",
    "# ì²« ë²ˆì§¸ ì€ë‹‰ì¸µ í™œì„±í™” ê°’ ê³„ì‚° (a1)\n",
    "Z1 = np.dot(W1, X) + b1  # Z1 = W1 * X + b1\n",
    "a1 = np.tanh(Z1)         # í™œì„±í™” í•¨ìˆ˜ë¡œ tanhë¥¼ ì‚¬ìš© (ì›í•˜ëŠ” í•¨ìˆ˜ë¡œ ë³€ê²½ ê°€ëŠ¥)\n",
    "\n",
    "# ì¶œë ¥ì¸µ: 1ê°œì˜ ë‰´ëŸ° (a2_1)\n",
    "# W[2]: (1, 2) í¬ê¸°ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ (1ê°œì˜ ì¶œë ¥ ë‰´ëŸ°, 2ê°œì˜ ì…ë ¥ ë‰´ëŸ°)\n",
    "W2 = np.random.randn(1, 2)\n",
    "\n",
    "# b[2]: (1, 1) í¬ê¸°ì˜ í¸í–¥ ë²¡í„° (1ê°œì˜ ì¶œë ¥ ë‰´ëŸ°ì— ëŒ€ì‘í•˜ëŠ” í¸í–¥)\n",
    "b2 = np.random.randn(1, 1)\n",
    "\n",
    "# ì¶œë ¥ì¸µ í™œì„±í™” ê°’ ê³„ì‚° (a2)\n",
    "Z2 = np.dot(W2, a1) + b2  # Z2 = W2 * a1 + b2\n",
    "a2 = 1 / (1 + np.exp(-Z2))  # í™œì„±í™” í•¨ìˆ˜ë¡œ ì‹œê·¸ëª¨ì´ë“œ ì‚¬ìš©\n",
    "\n",
    "# ìµœì¢… ì¶œë ¥\n",
    "y_hat = a2  # ì˜ˆì¸¡ê°’ y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X: ì…ë ¥ ë°ì´í„°, í¬ê¸°ëŠ” **(4, 1)**ì´ë©° 4ê°œì˜ ì…ë ¥ ë‰´ëŸ°ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "- W1: ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜, í¬ê¸°ëŠ” **(2, 4)**ì…ë‹ˆë‹¤. ì´ëŠ” 2ê°œì˜ ì¶œë ¥ ë‰´ëŸ°ê³¼ 4ê°œì˜ ì…ë ¥ ë‰´ëŸ°ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "- b1: ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì˜ í¸í–¥, í¬ê¸°ëŠ” **(2, 1)**ì…ë‹ˆë‹¤. ì´ëŠ” ê° ì¶œë ¥ ë‰´ëŸ°ì— ëŒ€ì‘í•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤.\n",
    "- a1: ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì˜ í™œì„±í™” ê°’, tanh í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤.\n",
    "- W2: ì¶œë ¥ì¸µì˜ ê°€ì¤‘ì¹˜, í¬ê¸°ëŠ” **(1, 2)**ì…ë‹ˆë‹¤. ì´ëŠ” 1ê°œì˜ ì¶œë ¥ ë‰´ëŸ°ê³¼ 2ê°œì˜ ì…ë ¥ ë‰´ëŸ°ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "- b2: ì¶œë ¥ì¸µì˜ í¸í–¥, í¬ê¸°ëŠ” **(1, 1)**ì…ë‹ˆë‹¤.\n",
    "- y_hat: ìµœì¢… ì¶œë ¥ ê°’ìœ¼ë¡œ, ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì§„ ë¶„ë¥˜ ë¬¸ì œì˜ ì˜ˆì¸¡ ê°’ì„ ê³„ì‚°í–ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°€ì¤‘ì¹˜ í–‰ë ¬ì˜ í¬ê¸°ëŠ” **(ì¶œë ¥ ë‰´ëŸ° ìˆ˜, ì…ë ¥ ë‰´ëŸ° ìˆ˜)**ì´ë¯€ë¡œ <br>\n",
    "ì…ë ¥: 4ê°œì˜ ì…ë ¥ ë‰´ëŸ° (ì¦‰, ğ‘¥1,ğ‘¥2,ğ‘¥3,ğ‘¥4x)<br>\n",
    "ì¶œë ¥: 2ê°œì˜ ë‰´ëŸ°<br>\n",
    "ğ‘Š[1] ì˜Â í¬ê¸° = (2,4) <br>\n",
    "<br><br>\n",
    "í¸í–¥ì€ ì¶œë ¥ ë‰´ëŸ°ì˜ ìˆ˜ì— í•˜ë‚˜ì”© ëŒ€ì‘ë˜ë¯€ë¡œ, í¸í–¥ ë²¡í„°ì˜ í¬ê¸°ëŠ” (ì¶œë ¥ ë‰´ëŸ° ìˆ˜, 1)<br>\n",
    "b[1]  ì˜Â í¬ê¸°= (2,1)<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì…ë ¥ ë‰´ëŸ°: ğ‘¥1, ğ‘¥2 ì¦‰ ì…ë ¥ ë‰´ëŸ°ì€ 2ê°œ<br>\n",
    "ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì˜ ë‰´ëŸ°: a1[1] ,a2[1] ,a3[1] ,a4[1]<br>\n",
    "â€‹<br><br>\n",
    "Z[1]ì˜ í¬ê¸°: ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ì— ëŒ€í•œ ê°€ì¤‘í•©ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.<br>\n",
    "ì…ë ¥ ë‰´ëŸ°ì´ 2ê°œì´ê³ , ì²«ë²ˆì§¸ ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ì´ 4ê°œ ì´ë¯€ë¡œ Z[1]ì˜ í¬ê¸°ëŠ” (4, m)<br>\n",
    "\n",
    "\n",
    "A[1]ì˜ í¬ê¸° = Z[1]ê³¼ ê°™ì€ í¬ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° ì‹œê°í™” (ì˜ˆ: scatter plot)\n",
    "plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ì˜ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì—¬ í¬ê¸° í™•ì¸\n",
    "import numpy as np\n",
    "\n",
    "X = np.random.randn(2, 400)  # (2, 400) í¬ê¸°ì˜ ì„ì˜ì˜ ë°ì´í„° ìƒì„±\n",
    "Y = np.random.randn(1, 400)  # (1, 400) í¬ê¸°ì˜ ì„ì˜ì˜ ë°ì´í„° ìƒì„±\n",
    "\n",
    "shape_X = X.shape\n",
    "shape_Y = Y.shape\n",
    "m = X.shape[1]  # í›ˆë ¨ ì˜ˆì œì˜ ìˆ˜ëŠ” ì—´ ê°œìˆ˜ë¡œ ê³„ì‚°\n",
    "\n",
    "print('The shape of X is: ' + str(shape_X))\n",
    "print('The shape of Y is: ' + str(shape_Y))\n",
    "print('I have m = %d training examples!' % (m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª…í™•í•œ ë°ì´í„°ê°€ ì´ë¯¸ ë¡œë“œëœ ìƒíƒœì—ì„œ shape í™•ì¸\n",
    "X, Y = load_planar_dataset()  # ë°ì´í„°ì…‹ ë¡œë“œ (ì˜ˆì‹œ)\n",
    "shape_X = X.shape             # Xì˜ í˜•íƒœë¥¼ ê°€ì ¸ì˜´\n",
    "shape_Y = Y.shape             # Yì˜ í˜•íƒœë¥¼ ê°€ì ¸ì˜´\n",
    "m = X.shape[1]                # í›ˆë ¨ ì˜ˆì œì˜ ê°œìˆ˜ëŠ” Xì˜ ì—´ ê°œìˆ˜ë¡œ ì •ì˜ë¨\n",
    "\n",
    "print('The shape of X is: ' + str(shape_X))  # Xì˜ í˜•íƒœ ì¶œë ¥\n",
    "print('The shape of Y is: ' + str(shape_Y))  # Yì˜ í˜•íƒœ ì¶œë ¥\n",
    "print('I have m = %d training examples!' % (m))  # í›ˆë ¨ ì˜ˆì œ ê°œìˆ˜ ì¶œë ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X.shape[0]ëŠ” ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì§• ìˆ˜(ì…ë ¥ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "Y.shape[0]ëŠ” ì¶œë ¥ ë°ì´í„°ì˜ í¬ê¸°(ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ê°œìˆ˜)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "ì€ë‹‰ì¸µì€ ë¬¸ì œì—ì„œ í•˜ë“œì½”ë”©í•˜ì—¬ 4ë¡œ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Argument'\n",
    "# n_x -- ì…ë ¥ì¸µì˜ í¬ê¸° (ì…ë ¥ì¸µì˜ ë…¸ë“œ ìˆ˜, Xì˜ ì²«ë²ˆì§¸ ì°¨ì›)\n",
    "# n_h -- ì€ë‹‰ì¸µì˜ í¬ê¸° (ì€ë‹‰ì¸µì˜ ë…¸ë“œ ìˆ˜)\n",
    "# n_y -- ì¶œë ¥ì¸µì˜ í¬ê¸° (ì¶œë ¥ì¸µì˜ ë…¸ë“œ ìˆ˜)\n",
    "\n",
    "W1 = np.random.randn(n_h, n_x) * 0.01  # ì‘ì€ ëœë¤ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "b1 = np.zeros((n_h, 1))                # 0ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "W2 = np.random.randn(n_y, n_h) * 0.01  # ì‘ì€ ëœë¤ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”\n",
    "b2 = np.zeros((n_y, 1))                # 0ìœ¼ë¡œ ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**W1**: ì…ë ¥ì¸µê³¼ ì€ë‹‰ì¸µ ê°„ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬ì´ë©°, í¬ê¸°ëŠ” (n_h, n_x)ì…ë‹ˆë‹¤. np.random.randn(n_h, n_x) * 0.01ì„ ì‚¬ìš©í•˜ì—¬ ì‘ì€ ëœë¤ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.<br>\n",
    "**b1**: ì€ë‹‰ì¸µì˜ í¸í–¥ ë²¡í„°, í¬ê¸°ëŠ” (n_h, 1), np.zeros((n_h, 1))ë¥¼ ì‚¬ìš©í•˜ì—¬ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•œë‹¤.<br>\n",
    "**W2**: ì€ë‹‰ì¸µê³¼ ì¶œë ¥ì¸µ ê°„ì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬, í¬ê¸°ëŠ” (n_y, n_h). np.random.randn(n_y, n_h) * 0.01ì„ ì‚¬ìš©í•˜ì—¬ ì‘ì€ ëœë¤ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”<br>\n",
    "**b2**: ì¶œë ¥ì¸µì˜ í¸í–¥ ë²¡í„°. í¬ê¸°ëŠ” (n_y, 1). np.zeros((n_y, 1))ë¥¼ ì‚¬ìš©í•˜ì—¬ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•œë‹¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"W1\": np.random.randn(3, 2),  # ì²« ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜\n",
    "    \"b1\": np.zeros((3, 1)),       # ì²« ë²ˆì§¸ ì¸µì˜ í¸í–¥\n",
    "    \"W2\": np.random.randn(1, 3),  # ë‘ ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜\n",
    "    \"b2\": np.zeros((1, 1))        # ë‘ ë²ˆì§¸ ì¸µì˜ í¸í–¥\n",
    "}\n",
    "\n",
    "# parameters ë”•ì…”ë„ˆë¦¬ì—ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ê°€ì ¸ì˜¤ëŠ” ë°©ì‹\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters ë”•ì…”ë„ˆë¦¬ ë‚´ìš©:\n",
    "{\n",
    "    'W1': array([[ 0.1, -1.2],\n",
    "                 [ 1.3,  0.5],\n",
    "                 [-0.4,  0.2]]),\n",
    "    'b1': array([[0.],\n",
    "                 [0.],\n",
    "                 [0.]]),\n",
    "    'W2': array([[ 0.3, -0.6,  0.7]]),\n",
    "    'b2': array([[0.]])\n",
    "}\n",
    "\n",
    "# W1 ê°€ì¤‘ì¹˜ ë°°ì—´:\n",
    "[[ 0.1 -1.2]\n",
    " [ 1.3  0.5]\n",
    " [-0.4  0.2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-1.34655   ,  1.00616122,  0.42421941],\n",
      "       [-0.35107087,  0.19093117,  0.02351876],\n",
      "       [ 0.46988999,  1.2635511 ,  0.32752464],\n",
      "       [-0.84803007, -0.1987923 ,  0.32245407]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 0.66591241, -0.35810343,  1.03054955,  0.22810726]]), 'b2': array([[0.]])}\n"
     ]
    }
   ],
   "source": [
    "# ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "import numpy as np\n",
    "\n",
    "# ë°°ì—´ ì´ˆê¸°í™”\n",
    "W1 = np.random.randn(4, 3)  # 4x3 í˜•íƒœì˜ ê°€ì¤‘ì¹˜ ë°°ì—´\n",
    "b1 = np.zeros((4, 1))       # 4x1 í˜•íƒœì˜ í¸í–¥ ë°°ì—´\n",
    "W2 = np.random.randn(1, 4)  # 1x4 í˜•íƒœì˜ ê°€ì¤‘ì¹˜ ë°°ì—´\n",
    "b2 = np.zeros((1, 1))       # 1x1 í˜•íƒœì˜ í¸í–¥ ë°°ì—´\n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ ìƒì„±\n",
    "parameters = {\n",
    "    \"W1\": W1,  # ê°€ì¤‘ì¹˜ ë°°ì—´ì„ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "    \"b1\": b1,  # í¸í–¥ ë°°ì—´ì„ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥\n",
    "    \"W2\": W2,  # ê°€ì¤‘ì¹˜ ë°°ì—´\n",
    "    \"b2\": b2   # í¸í–¥ ë°°ì—´\n",
    "}\n",
    "\n",
    "# ë”•ì…”ë„ˆë¦¬ ì¶œë ¥\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Z1, A1, Z2, A2**ëŠ” ì‹ ê²½ë§ì˜ ì „ë°© ì „íŒŒ(Forward Propagation) ë‹¨ê³„ì—ì„œ ê° ì¸µì—ì„œ ê³„ì‚°ë˜ëŠ” ê°’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z1 = np.dot(W1, X) + b1\n",
    "# Z1ì€ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì˜ ì„ í˜• ë³€í™˜ ê°’ì„ ê³„ì‚°í•œ ê²°ê³¼\n",
    "# W1ì€ ì²« ë²ˆì§¸ ì€ë‹‰ì¸µì˜ ê°€ì¤‘ì¹˜ í–‰ë ¬\n",
    "# XëŠ” ì…ë ¥ ë°ì´í„°\n",
    "# ì´ ë‘˜ì„ í–‰ë ¬ ê³±(np.dot)í•œ í›„ì— í¸í–¥ b1ì„ ë”í•œ ê°’ì´ Z1 / ì•„ì§ í™œì„±í™” í•¨ìˆ˜ê°€ ì ìš©ë˜ì§€ ì•Šì€ ëª¨ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = np.tanh(Z1)\n",
    "# A1ì€ Z1ì— í™œì„±í™” í•¨ìˆ˜ì¸ tanhë¥¼ ì ìš©í•œ ê°’. -1 ê³¼ 1ì‚¬ì´ë¡œ ë³€í™˜í•˜ëŠ” ë¹„ì„ í˜• í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z2 = np.dot(W2, A1) + b2\n",
    "# Z2ëŠ” ì²« ë²ˆì§¸ ì¶œë ¥ì¸µì˜ ì„ í˜• ë³€í™˜ ê°’ì„ ê³„ì‚°í•œ ê²°ê³¼\n",
    "# W2ëŠ” ë‘ ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜, A1ì€ ì²« ë²ˆì§¸ ì…ë ¥ê°’ì„ ì¶œë ¥ê°’, ì´ ë‘˜ì„ í–‰ë ¬ ê³± í•œ í›„ì— í¸í–¥ b2ë¥¼ ë”í•œ ê°’ì´ Z2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = sigmoid(Z2)\n",
    "# A2ëŠ” Z2ì— ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•œ ê°’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°ë²•\n",
    "$$\n",
    "ğ½=âˆ’1ğ‘šâˆ‘ğ‘–=1ğ‘š(ğ‘¦(ğ‘–)log(ğ‘[2](ğ‘–))+(1âˆ’ğ‘¦(ğ‘–))log(1âˆ’ğ‘[2](ğ‘–)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))\n",
    "cost = - np.sum(logprobs) / m\n",
    "cost = float(np.squeeze(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logprobs = np.dot(Y, np.log(A2).T) + np.dot((1 - Y), np.log(1 - A2).T)\n",
    "cost = - logprobs / m  # í‰ê· í™”ëœ ë¹„ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8776)\n"
     ]
    }
   ],
   "source": [
    "# íŒŒì´í† ì¹˜ ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’\n",
    "predictions = torch.sigmoid(torch.randn(10, 1))  # ì˜ˆì¸¡ê°’\n",
    "targets = torch.rand(10, 1)  # ì‹¤ì œê°’\n",
    "\n",
    "# ì†ì‹¤ ê³„ì‚°\n",
    "loss = loss_fn(predictions, targets)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.7014014, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# í…ì„œí”Œë¡œ ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°\n",
    "import tensorflow as tf\n",
    "\n",
    "# êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’\n",
    "predictions = tf.random.normal([10, 1])  # ì˜ˆì¸¡ê°’\n",
    "targets = tf.random.uniform([10, 1], minval=0, maxval=1)  # ì‹¤ì œê°’\n",
    "\n",
    "# ì†ì‹¤ ê³„ì‚°\n",
    "loss = loss_fn(targets, tf.sigmoid(predictions))\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì†ì‹¤ í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5 - ì—­ì „íŒŒ(Backpropagation) êµ¬í˜„\n",
    "ì „ë°© ì „íŒŒ(Forward Propagation) ë™ì•ˆ ê³„ì‚°ëœ **ìºì‹œ(cache)**ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì œ ì—­ì „íŒŒë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì—­ì „íŒŒëŠ” **ê¸°ìš¸ê¸°(gradient)**ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, ê° ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì— ëŒ€í•œ ì˜¤ì°¨ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ì—¬ ì‹ ê²½ë§ í•™ìŠµì—ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤. <br>\n",
    "ìŠ¬ë¼ì´ë“œì—ì„œ ì˜¤ë¥¸ìª½ì— ë‚˜ì™€ ìˆëŠ” ë°©ì •ì‹ë“¤ì€ ì—­ì „íŒŒ ê³„ì‚°ì— í•„ìš”í•œ ìˆ˜ì‹ì„ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤. ì´ ë°©ì •ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê° ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.<br>\n",
    "ì „ë°© ì „íŒŒì—ì„œ ê³„ì‚°ëœ ê°’ë“¤ì„ ì‚¬ìš©í•˜ì—¬ ì—­ì „íŒŒì—ì„œ í•„ìš”í•œ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìºì‹œ ë”•ì…”ë„ˆë¦¬ ì˜ˆì‹œ\n",
    "cache = {\n",
    "    \"Z1\": Z1,  # ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ì„ í˜• ë³€í™˜ ê²°ê³¼\n",
    "    \"A1\": A1,  # ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ í™œì„±í™” í•¨ìˆ˜ ì ìš© ê²°ê³¼\n",
    "    \"Z2\": Z2,  # ë‘ ë²ˆì§¸ ë ˆì´ì–´ì˜ ì„ í˜• ë³€í™˜ ê²°ê³¼\n",
    "    \"A2\": A2   # ìµœì¢… ì¶œë ¥ (ì˜ˆì¸¡ê°’)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parametersëŠ” **ì „ë°© ì „íŒŒ(Forward Propagation)**ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í•„ìš”í•œ **ê°€ì¤‘ì¹˜(W)**ì™€ í¸í–¥(b) ê°™ì€ ëª¨ë¸ì˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ì €ì¥í•œ ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤.<br> \n",
    "ì´ ê°’ë“¤ì€ í•™ìŠµ ê³¼ì •ì—ì„œ ì¡°ì •ë˜ë©°, ëª¨ë¸ì´ ë°ì´í„°ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë•ëŠ” ì¤‘ìš”í•œ ìš”ì†Œë“¤<br>\n",
    "\n",
    "cacheëŠ” **ì—­ì „íŒŒ(Backpropagation)**ë¥¼ ìˆ˜í–‰í•  ë•Œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì „ë°© ì „íŒŒì—ì„œ ê³„ì‚°ëœ ì¤‘ê°„ ê°’ë“¤(ì˜ˆ: Z1, A1, Z2, A2)ì„ ì €ì¥í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ì…ë‹ˆë‹¤. <br>\n",
    "ì´ ê°’ë“¤ì€ ê¸°ìš¸ê¸° ê³„ì‚°ì„ ìœ„í•´ í•„ìš”í•˜ë©°, í•™ìŠµ ê³¼ì •ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì „ë°© ì „íŒŒ(Forward Propagation): parametersì—ì„œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë¶ˆëŸ¬ì™€ ê³„ì‚°ì„ ìˆ˜í–‰í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ cacheì— ì €ì¥í•©ë‹ˆë‹¤. <br>\n",
    "ì—­ì „íŒŒ(Backpropagation): cacheì— ì €ì¥ëœ ê°’ë“¤ì„ ì‚¬ìš©í•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ê³ , ì´ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ parametersì— ì €ì¥ëœ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì—­ì „íŒŒ êµ¬í˜„\n",
    "'1. parametersì—ì„œ W1, W2 ê°€ì ¸ì˜¤ê¸°'\n",
    "W1 = parameters[\"W1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "\n",
    "'2. cacheì—ì„œ A1, A2 ê°€ì ¸ì˜¤ê¸°'\n",
    "A1 = cache[\"A1\"]\n",
    "A2 = cache[\"A2\"]\n",
    "\n",
    "'3. ê¸°ìš¸ê¸° ê³„ì‚°'\n",
    "dZ2 = A2 - Y # ì¶œë ¥ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ê³„ì‚°.\n",
    "dW2 = (1 / m) * np.dot(dZ2, A1.T) # ë‘ ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê¸°ìš¸ê¸° ê³„ì‚°.\n",
    "db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True) # ë‘ ë²ˆì§¸ ì¸µì˜ í¸í–¥ì— ëŒ€í•œ ê¸°ìš¸ê¸° ê³„ì‚°.\n",
    "dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)) # ì²« ë²ˆì§¸ ì¸µì—ì„œì˜ í™œì„±í™” í•¨ìˆ˜ ë¯¸ë¶„ì„ ê³ ë ¤í•œ ì˜¤ì°¨ ê³„ì‚°.\n",
    "\n",
    "dW1 = (1 / m) * np.dot(dZ1, X.T) # ì²« ë²ˆì§¸ ì¸µì˜ ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê¸°ìš¸ê¸° ê³„ì‚°.\n",
    "db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True) # ì²« ë²ˆì§¸ ì¸µì˜ í¸í–¥ì— ëŒ€í•œ ê¸°ìš¸ê¸° ê³„ì‚°.\n",
    "\n",
    "'ëª¨ë“  ê¸°ìš¸ê¸°ë¥¼ grads ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'ëª¨ë“  ê¸°ìš¸ê¸°ë¥¼ grads ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.'\n",
    "grads = {\"dW1\": dW1,  # W1ì˜ ê¸°ìš¸ê¸°\n",
    "         \"db1\": db1,  # b1ì˜ ê¸°ìš¸ê¸°\n",
    "         \"dW2\": dW2,  # W2ì˜ ê¸°ìš¸ê¸°\n",
    "         \"db2\": db2}  # b2ì˜ ê¸°ìš¸ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-6 ê²½ì‚¬ í•˜ê°•ë²• Gradient Descent \n",
    "ì´ ê³µì‹ì€ íŒŒë¼ë¯¸í„° Î¸ë¥¼ ì¡°ê¸ˆì”© ì—…ë°ì´íŠ¸í•˜ì—¬ ì†ì‹¤ í•¨ìˆ˜ J(Î¸)ê°€ ìµœì†Œí™”ë˜ë„ë¡ í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. <br>\n",
    "ê¸°ìš¸ê¸°ë¥¼ ë”°ë¼ì„œ ë‚´ë ¤ê°€ëŠ” ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•´ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ë©°, íŒŒë¼ë¯¸í„°ë¥¼ ê³„ì†í•´ì„œ ìˆ˜ì •í•˜ë©´ì„œ ìµœì ì˜ ê°’ì„ ì°¾ì•„ê°‘ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'1. íŒŒë¼ë¯¸í„°ì™€ ê¸°ìš¸ê¸° ê°€ì ¸ì˜¤ê¸°'\n",
    "parameters, grads = update_parameters_test_case()\n",
    "# parametersì—ëŠ” ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ W1, W2ì™€ í¸í–¥ b1, b2ê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n",
    "# gradsì—ëŠ” ê²½ì‚¬ í•˜ê°•ë²•ì—ì„œ ê³„ì‚°ëœ dW1, dW2, db1, db2ë¼ëŠ” ê¸°ìš¸ê¸°ê°€ ë“¤ì–´ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'2. ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸'\n",
    "parameters = update_parameters(parameters, grads)\n",
    "# ì´ì „ì— ì €ì¥ëœ parameters ë”•ì…”ë„ˆë¦¬ë¡œë¶€í„° ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ë³µì‚¬í•©ë‹ˆë‹¤ (copy.deepcopy()).\n",
    "# gradsì—ì„œ ê¸°ìš¸ê¸° ê°’ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\n",
    "\n",
    "'ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•´ ìƒˆë¡œìš´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ê³„ì‚°í•˜ê³ , ì´ ê°’ì„ ë”•ì…”ë„ˆë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤.'\n",
    "W1 = W1 - learning_rate * dW1\n",
    "b1 = b1 - learning_rate * db1\n",
    "W2 = W2 - learning_rate * dW2\n",
    "b2 = b2 - learning_rate * db2\n",
    "# í•™ìŠµë¥  learning_rateì— ë”°ë¼ ê¸°ìš¸ê¸°ë§Œí¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤\n",
    "\n",
    "'ì—…ë°ì´íŠ¸ëœ parametersë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`for`ë¬¸ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì‹ ê²½ë§ í•™ìŠµ ê³¼ì •ì—ì„œ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì´ **ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ**ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. **ê²½ì‚¬ í•˜ê°•ë²•**ì€ ë°˜ë³µì ì¸ ê³¼ì •ì„ í†µí•´ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ê³ , ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì—…ë°ì´íŠ¸í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì—¬ëŸ¬ ë²ˆì˜ ë°˜ë³µì´ í•„ìš”í•˜ë©°, ì´ë¥¼ ìœ„í•´ `for`ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ **ì§€ì •ëœ íšŸìˆ˜ë§Œí¼** í•™ìŠµì„ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
    "\n",
    "ì´ì „ì— `for`ë‚˜ `while`ë¬¸ì„ ì‚¬ìš©í•˜ì§€ ë§ë¼ëŠ” ì¡°ê±´ì€ íŠ¹ì • ë¬¸ì œì— ëŒ€í•œ ì œì•½ì´ì—ˆì„ ìˆ˜ ìˆì§€ë§Œ, ì‹ ê²½ë§ í•™ìŠµ ê³¼ì •ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë°˜ë³µì´ í•„ìš”í•œ ì‘ì—…ì´ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ì˜ˆì™¸ì ìœ¼ë¡œ `for`ë¬¸ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "### ë¬¸ì œ í’€ì´:\n",
    "\n",
    "ì´ì œ `nn_model()` í•¨ìˆ˜ì˜ ë¬¸ì œë¥¼ í•˜ë‚˜ì”© í•´ê²°í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "```python\n",
    "# GRADED FUNCTION: nn_model\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)  # ëœë¤ ì‹œë“œ ì„¤ì •\n",
    "    n_x = layer_sizes(X, Y)[0]  # ì…ë ¥ì¸µ í¬ê¸°\n",
    "    n_y = layer_sizes(X, Y)[2]  # ì¶œë ¥ì¸µ í¬ê¸°\n",
    "    \n",
    "    # íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # ê²½ì‚¬ í•˜ê°•ë²• ë£¨í”„\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        # 1. ì „ë°© ì „íŒŒ\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # 2. ë¹„ìš© í•¨ìˆ˜ ê³„ì‚°\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # 3. ì—­ì „íŒŒ\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        \n",
    "        # 4. íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        # ë§¤ 1000ë²ˆì˜ ë°˜ë³µë§ˆë‹¤ ë¹„ìš© ì¶œë ¥\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "\n",
    "    return parameters\n",
    "```\n",
    "\n",
    "### ì½”ë“œ ì„¤ëª…:\n",
    "\n",
    "1. **íŒŒë¼ë¯¸í„° ì´ˆê¸°í™”** (`initialize_parameters()`):\n",
    "   ```python\n",
    "   parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "   ```\n",
    "   ì—¬ê¸°ì„œ ì…ë ¥ì¸µ(`n_x`), ì€ë‹‰ì¸µ(`n_h`), ì¶œë ¥ì¸µ(`n_y`)ì˜ í¬ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "2. **ê²½ì‚¬ í•˜ê°•ë²• ë£¨í”„**:\n",
    "   - í•™ìŠµ ê³¼ì •ì€ `num_iterations` ë§Œí¼ ë°˜ë³µë˜ë©°, ê° ë°˜ë³µë§ˆë‹¤ ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.\n",
    "\n",
    "3. **ì „ë°© ì „íŒŒ** (`forward_propagation()`):\n",
    "   ```python\n",
    "   A2, cache = forward_propagation(X, parameters)\n",
    "   ```\n",
    "   - ì…ë ¥ ë°ì´í„° `X`ì™€ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì „ë°© ì „íŒŒë¥¼ í†µí•´ **ì¶œë ¥ ê°’**ê³¼ ì¤‘ê°„ ê³„ì‚° ê°’(`cache`)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "4. **ë¹„ìš© í•¨ìˆ˜ ê³„ì‚°** (`compute_cost()`):\n",
    "   ```python\n",
    "   cost = compute_cost(A2, Y)\n",
    "   ```\n",
    "   - ì˜ˆì¸¡ëœ ì¶œë ¥ ê°’ `A2`ì™€ ì‹¤ì œ ê°’ `Y`ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ìš©(ì†ì‹¤)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "\n",
    "5. **ì—­ì „íŒŒ** (`backward_propagation()`):\n",
    "   ```python\n",
    "   grads = backward_propagation(parameters, cache, X, Y)\n",
    "   ```\n",
    "   - ì „ë°© ì „íŒŒì—ì„œ ê³„ì‚°ëœ ì¤‘ê°„ ê°’ê³¼ ë¹„ìš©ì„ ì‚¬ìš©í•´ ê¸°ìš¸ê¸°(gradient)ë¥¼ ê³„ì‚°í•˜ì—¬ ì—­ì „íŒŒë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "\n",
    "6. **íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸** (`update_parameters()`):\n",
    "   ```python\n",
    "   parameters = update_parameters(parameters, grads)\n",
    "   ```\n",
    "   - ì—­ì „íŒŒë¡œ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ì‚¬ìš©í•´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì„ ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.\n",
    "\n",
    "7. **ë¹„ìš© ì¶œë ¥**:\n",
    "   ```python\n",
    "   if print_cost and i % 1000 == 0:\n",
    "       print(\"Cost after iteration %i: %f\" % (i, cost))\n",
    "   ```\n",
    "   - `print_cost`ê°€ `True`ì¼ ë•Œ, ë§¤ 1000ë²ˆì˜ ë°˜ë³µë§ˆë‹¤ ë¹„ìš©ì„ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
    "\n",
    "### forë¬¸ì„ ì‚¬ìš©í•˜ëŠ” ì´ìœ :\n",
    "- **ë°˜ë³µì ìœ¼ë¡œ** ì „ë°© ì „íŒŒ â†’ ë¹„ìš© ê³„ì‚° â†’ ì—­ì „íŒŒ â†’ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ ê³¼ì •ì´ ì§„í–‰ë˜ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— `for`ë¬¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "- ë°˜ë³µë¬¸ ì—†ì´ ê²½ì‚¬ í•˜ê°•ë²•ì„ í•œ ë²ˆë§Œ ì ìš©í•˜ë©´ ëª¨ë¸ì´ ì¶©ë¶„íˆ í•™ìŠµë˜ì§€ ì•Šê¸° ë•Œë¬¸ì—, `for`ë¬¸ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì ìš©í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ ì´ í•¨ìˆ˜ëŠ” **ì‹ ê²½ë§ í•™ìŠµ ê³¼ì •**ì„ í†µí•©í•˜ì—¬ `nn_model` í•¨ìˆ˜ë¥¼ í†µí•´ í•™ìŠµí•˜ê³ , í•™ìŠµëœ íŒŒë¼ë¯¸í„°ë¥¼ ë°˜í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A2, cache = forward_propagation(X, parameters) # ì „ë°© ì „íŒŒë¥¼ ì‚¬ìš©í•˜ì—¬ A2 ê³„ì‚°\n",
    "predictions = (A2 > 0.5) # ì„ê³„ê°’ ì ìš©: A2ì˜ ê°’ì´ 0.5ë³´ë‹¤ í¬ë©´ True, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ Falseê°€ ë˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ ê°’ì„ 0ê³¼ 1ë¡œ ë³€í™˜í•˜ì—¬ ì´ì§„ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì‹ ê²½ë§ êµ¬ì¶• ê³¼ì •"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. íŒ¨í‚¤ì§€ ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. ë°ì´í„°ì…‹ ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# í‰ë©´ ë°ì´í„°ì…‹ ìƒì„±\n",
    "X, Y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# numpy -> torch tensor ë³€í™˜\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32).unsqueeze(1)  # Yê°€ (batch_size, 1) í˜•íƒœë¡œ ë§Œë“¤ì–´ì§\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜\n",
    "PyTorchì—ì„œëŠ” nn.Moduleì„ ìƒì†ë°›ì•„ ì‹ ê²½ë§ì„ ì •ì˜í•©ë‹ˆë‹¤. PyTorchì—ì„œëŠ” ì´ë¥¼ í´ë˜ìŠ¤ì˜ forward ë©”ì†Œë“œë¡œ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)  # ì€ë‹‰ì¸µ\n",
    "        self.output = nn.Linear(hidden_size, output_size)  # ì¶œë ¥ì¸µ\n",
    "        self.activation = nn.Tanh()  # í™œì„±í™” í•¨ìˆ˜ (tanh)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z1 = self.hidden(x)          # ì „ë°© ì „íŒŒ: ì€ë‹‰ì¸µ\n",
    "        a1 = self.activation(z1)     # ì€ë‹‰ì¸µ í™œì„±í™” í•¨ìˆ˜ ì ìš©\n",
    "        z2 = self.output(a1)         # ì¶œë ¥ì¸µìœ¼ë¡œ ì „ë°© ì „íŒŒ\n",
    "        return torch.sigmoid(z2)     # ì´ì§„ ë¶„ë¥˜ì—ì„œ sigmoid í•¨ìˆ˜ ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ì†ì‹¤ í•¨ìˆ˜ ë° ìµœì í™”ê¸° ì •ì˜\n",
    "ì´ì œ ì†ì‹¤ í•¨ìˆ˜ë¡œ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤(cross-entropy loss)ì„ ì‚¬ìš©í•˜ê³ , ìµœì í™” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²½ì‚¬ í•˜ê°•ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "\n",
    "PyTorchì—ì„œëŠ” torch.optim ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬ ì´ë¥¼ ê°„í¸í•˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ì´ˆê¸°í™”\n",
    "input_size = 2  # ë°ì´í„°ì…‹ì˜ íŠ¹ì„± ìˆ˜ (Xê°€ 2ì°¨ì›)\n",
    "hidden_size = 4  # ì€ë‹‰ì¸µ í¬ê¸° (ì„ íƒ ì‚¬í•­)\n",
    "output_size = 1  # ì¶œë ¥ í¬ê¸° (ì´ì§„ ë¶„ë¥˜)\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™”ê¸° ì •ì˜\n",
    "criterion = nn.BCELoss()  # ì´ì§„ ë¶„ë¥˜ì— ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # ê²½ì‚¬ í•˜ê°•ë²• (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. í›ˆë ¨ ë£¨í”„\n",
    "ì´ì œ í›ˆë ¨ ë£¨í”„ë¥¼ ì •ì˜í•˜ì—¬ ë°ì´í„°ë¥¼ í•™ìŠµì‹œí‚¤ê³ , ë§¤ ë°˜ë³µë§ˆë‹¤ ì†ì‹¤ ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤. PyTorchëŠ” ìë™ ë¯¸ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ê¸°ìš¸ê¸° ê³„ì‚°ì„ ìë™ìœ¼ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ, ìˆ˜ë™ìœ¼ë¡œ ì—­ì „íŒŒ(backpropagation)ë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5000/100000], Loss: 0.2859\n",
      "Epoch [10000/100000], Loss: 0.2845\n",
      "Epoch [15000/100000], Loss: 0.2814\n",
      "Epoch [20000/100000], Loss: 0.2585\n",
      "Epoch [25000/100000], Loss: 0.1714\n",
      "Epoch [30000/100000], Loss: 0.1287\n",
      "Epoch [35000/100000], Loss: 0.1095\n",
      "Epoch [40000/100000], Loss: 0.0995\n",
      "Epoch [45000/100000], Loss: 0.0936\n",
      "Epoch [50000/100000], Loss: 0.0899\n",
      "Epoch [55000/100000], Loss: 0.0873\n",
      "Epoch [60000/100000], Loss: 0.0854\n",
      "Epoch [65000/100000], Loss: 0.0840\n",
      "Epoch [70000/100000], Loss: 0.0829\n",
      "Epoch [75000/100000], Loss: 0.0820\n",
      "Epoch [80000/100000], Loss: 0.0812\n",
      "Epoch [85000/100000], Loss: 0.0806\n",
      "Epoch [90000/100000], Loss: 0.0800\n",
      "Epoch [95000/100000], Loss: 0.0795\n",
      "Epoch [100000/100000], Loss: 0.0791\n"
     ]
    }
   ],
   "source": [
    "# í›ˆë ¨ ë£¨í”„\n",
    "num_epochs = 100000\n",
    "for epoch in range(num_epochs):\n",
    "    # ìˆœì „íŒŒ (forward propagation)\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    \n",
    "    # ì—­ì „íŒŒ (backward propagation)\n",
    "    optimizer.zero_grad()  # ê¸°ìš¸ê¸° ì´ˆê¸°í™”\n",
    "    loss.backward()  # ì—­ì „íŒŒ ìë™ ê³„ì‚°\n",
    "    optimizer.step()  # ê²½ì‚¬ í•˜ê°•ë²•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸\n",
    "    \n",
    "    # 100ë²ˆì§¸ ì—í¬í¬ë§ˆë‹¤ ì†ì‹¤ ì¶œë ¥\n",
    "    if (epoch+1) % 5000 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. ëª¨ë¸ í‰ê°€\n",
    "ì¶œë ¥ê°’ì´ 0.5ë³´ë‹¤ í° ê²½ìš° 1ë¡œ, ê·¸ë ‡ì§€ ì•Šì€ ê²½ìš° 0ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 97.00%\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():  # ê¸°ìš¸ê¸° ê³„ì‚°ì„ í•˜ì§€ ì•ŠìŒ\n",
    "    test_outputs = model(X_test)\n",
    "    predicted = (test_outputs > 0.5).float()\n",
    "    accuracy = (predicted == Y_test).float().mean().item()  # Booleanì„ floatìœ¼ë¡œ ë³€í™˜\n",
    "    print(f'Accuracy on test data: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ ì½”ë“œ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ë°ì´í„°ì…‹ ìƒì„±\n",
    "X, Y = make_moons(n_samples=1000, noise=0.2, random_state=42)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# numpy -> torch tensor ë³€í™˜\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "Y_train = torch.tensor(Y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "Y_test = torch.tensor(Y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# ì‹ ê²½ë§ ëª¨ë¸ ì •ì˜\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.output = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        z1 = self.hidden(x)\n",
    "        a1 = self.activation(z1)\n",
    "        z2 = self.output(a1)\n",
    "        return torch.sigmoid(z2)\n",
    "\n",
    "# ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ìµœì í™”ê¸° ì´ˆê¸°í™”\n",
    "input_size = 2\n",
    "hidden_size = 4\n",
    "output_size = 1\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# í›ˆë ¨ ë£¨í”„\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, Y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# ëª¨ë¸ í‰ê°€\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    predicted = (test_outputs > 0.5).float()\n",
    "    accuracy = (predicted == Y_test).mean().item()\n",
    "    print(f'Accuracy on test data: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
